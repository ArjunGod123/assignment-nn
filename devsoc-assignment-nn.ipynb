{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nclass Linear:\n    def __init__(self, in_features, out_features):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.biases = np.zeros((1, out_features))\n        self.input = None\n        self.grad_weights = None\n        self.grad_biases = None\n\n    def forward(self, x):\n        self.input = x\n        return np.dot(x, self.weights) + self.biases\n\n    def backward(self, d_out):\n        self.grad_weights = np.dot(self.input.T, d_out)\n        self.grad_biases = np.sum(d_out, axis=0, keepdims=True)\n        d_input = np.dot(d_out, self.weights.T)\n        return d_input\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-11T18:17:13.938221Z","iopub.execute_input":"2024-08-11T18:17:13.938860Z","iopub.status.idle":"2024-08-11T18:17:13.950711Z","shell.execute_reply.started":"2024-08-11T18:17:13.938793Z","shell.execute_reply":"2024-08-11T18:17:13.949272Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"class ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n\n    def backward(self, d_out):\n        d_input = d_out.copy()\n        d_input[self.input <= 0] = 0\n        return d_input\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:13.953301Z","iopub.execute_input":"2024-08-11T18:17:13.953666Z","iopub.status.idle":"2024-08-11T18:17:13.967606Z","shell.execute_reply.started":"2024-08-11T18:17:13.953635Z","shell.execute_reply":"2024-08-11T18:17:13.966341Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class Sigmoid:\n    def forward(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (self.output * (1 - self.output))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:13.969069Z","iopub.execute_input":"2024-08-11T18:17:13.969418Z","iopub.status.idle":"2024-08-11T18:17:13.981687Z","shell.execute_reply.started":"2024-08-11T18:17:13.969389Z","shell.execute_reply":"2024-08-11T18:17:13.980403Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"class Tanh:\n    def forward(self, x):\n        self.output = np.tanh(x)\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (1 - self.output ** 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:13.983315Z","iopub.execute_input":"2024-08-11T18:17:13.984430Z","iopub.status.idle":"2024-08-11T18:17:13.995455Z","shell.execute_reply.started":"2024-08-11T18:17:13.984370Z","shell.execute_reply":"2024-08-11T18:17:13.993989Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"class Softmax:\n    def forward(self, x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_out):\n        # Assuming d_out is already the gradient of loss w.r.t. softmax output\n        return d_out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:13.998387Z","iopub.execute_input":"2024-08-11T18:17:13.998799Z","iopub.status.idle":"2024-08-11T18:17:14.009343Z","shell.execute_reply.started":"2024-08-11T18:17:13.998768Z","shell.execute_reply":"2024-08-11T18:17:14.007790Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"class CrossEntropyLoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        self.n = y_true.shape[0]\n        loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / self.n\n        return loss\n\n    def backward(self):\n        return (self.y_pred - self.y_true) / self.y_true.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:14.011045Z","iopub.execute_input":"2024-08-11T18:17:14.011527Z","iopub.status.idle":"2024-08-11T18:17:14.027781Z","shell.execute_reply.started":"2024-08-11T18:17:14.011483Z","shell.execute_reply":"2024-08-11T18:17:14.026179Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"class MSELoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        return np.mean((y_pred - y_true) ** 2)\n\n    def backward(self):\n        return 2 * (self.y_pred - self.y_true) / self.y_true.size\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:14.029521Z","iopub.execute_input":"2024-08-11T18:17:14.030017Z","iopub.status.idle":"2024-08-11T18:17:14.041548Z","shell.execute_reply.started":"2024-08-11T18:17:14.029956Z","shell.execute_reply":"2024-08-11T18:17:14.040206Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"class SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        if hasattr(layer, 'grad_weights'):\n            layer.weights -= self.learning_rate * layer.grad_weights\n            layer.biases -= self.learning_rate * layer.grad_biases\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:14.057177Z","iopub.execute_input":"2024-08-11T18:17:14.057590Z","iopub.status.idle":"2024-08-11T18:17:14.065420Z","shell.execute_reply.started":"2024-08-11T18:17:14.057558Z","shell.execute_reply":"2024-08-11T18:17:14.063817Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_fn = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self, loss_fn, optimizer):\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n\n    def backward(self, loss_grad):\n        for layer in reversed(self.layers):\n            loss_grad = layer.backward(loss_grad)\n\n    def train(self, x_train, y_train, epochs, batch_size):\n        for epoch in range(epochs):\n            # Shuffle training data\n            indices = np.arange(x_train.shape[0])\n            np.random.shuffle(indices)\n            x_train, y_train = x_train[indices], y_train[indices]\n            \n            # Mini-batch training\n            for start in range(0, x_train.shape[0], batch_size):\n                end = min(start + batch_size, x_train.shape[0])\n                x_batch, y_batch = x_train[start:end], y_train[start:end]\n                \n                # Forward pass\n                predictions = self.forward(x_batch)\n                \n                # Compute loss\n                loss = self.loss_fn.forward(predictions, y_batch)\n                \n                # Backward pass\n                loss_grad = self.loss_fn.backward()\n                self.backward(loss_grad)\n                \n                # Update parameters\n                for layer in self.layers:\n                    self.optimizer.step(layer)\n                \n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n\n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss_fn.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n\n    def save(self, filename):\n        # Implement model saving\n        pass\n\n    def load(self, filename):\n        # Implement model loading\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:14.068431Z","iopub.execute_input":"2024-08-11T18:17:14.069645Z","iopub.status.idle":"2024-08-11T18:17:14.086388Z","shell.execute_reply.started":"2024-08-11T18:17:14.069580Z","shell.execute_reply":"2024-08-11T18:17:14.084924Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Separate features and labels\nx = train_df.drop(columns='label').values\ny = train_df['label'].values\n\n# Normalize pixel values\nx = x / 255.0\n\n# Reshape data to (num_samples, 28, 28)\nx = x.reshape(-1, 28, 28)\n\n# One-hot encode the labels\ny = to_categorical(y, 10)\n\n# Flatten the data\nx = x.reshape(x.shape[0], -1)\n\n# Split data into training and validation sets\n\nnp.random.seed(42)  # For reproducibility\nindices = np.arange(x.shape[0])\nnp.random.shuffle(indices)\nsplit_index = int(0.8 * len(indices))\n\ntrain_indices = indices[:split_index]\nval_indices = indices[split_index:]\n\nx_train, x_val = x[train_indices], x[val_indices]\ny_train, y_val = y[train_indices], y[val_indices]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:14.087943Z","iopub.execute_input":"2024-08-11T18:17:14.088384Z","iopub.status.idle":"2024-08-11T18:17:17.877278Z","shell.execute_reply.started":"2024-08-11T18:17:14.088348Z","shell.execute_reply":"2024-08-11T18:17:17.875907Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Initialize model and layers\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\n# Compile model with loss and optimizer\nloss_fn = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.5)\nmodel.compile(loss_fn, optimizer)\n\n# Train the model\nmodel.train(x_train, y_train, epochs=25, batch_size=64)\n\n# Evaluate the model on the validation dataset\nval_loss, val_accuracy = model.evaluate(x_val, y_val)\nprint(f'Validation Loss: {val_loss}')\nprint(f'Validation Accuracy: {val_accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:17:17.878896Z","iopub.execute_input":"2024-08-11T18:17:17.879249Z","iopub.status.idle":"2024-08-11T18:18:01.340568Z","shell.execute_reply.started":"2024-08-11T18:17:17.879219Z","shell.execute_reply":"2024-08-11T18:18:01.339302Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Epoch 1/25, Loss: 0.21772892976406097\nEpoch 2/25, Loss: 0.11674677436346408\nEpoch 3/25, Loss: 0.25393391405482685\nEpoch 4/25, Loss: 0.011720801745656938\nEpoch 5/25, Loss: 0.10206746395636827\nEpoch 6/25, Loss: 0.08921165988671861\nEpoch 7/25, Loss: 0.017043194145259163\nEpoch 8/25, Loss: 0.012681621194170118\nEpoch 9/25, Loss: 0.009078825226183446\nEpoch 10/25, Loss: 0.012021268714607356\nEpoch 11/25, Loss: 0.008234270360459834\nEpoch 12/25, Loss: 0.004146135683986552\nEpoch 13/25, Loss: 0.011501410704180554\nEpoch 14/25, Loss: 0.013664044868317767\nEpoch 15/25, Loss: 0.00487180073110783\nEpoch 16/25, Loss: 0.002653488353319305\nEpoch 17/25, Loss: 0.004305511846852729\nEpoch 18/25, Loss: 0.0014052776089672095\nEpoch 19/25, Loss: 0.0014816231449170814\nEpoch 20/25, Loss: 0.0011875976003857813\nEpoch 21/25, Loss: 0.0009559408802894135\nEpoch 22/25, Loss: 0.0013809006818809293\nEpoch 23/25, Loss: 0.0015250293505149903\nEpoch 24/25, Loss: 0.0010486909522863452\nEpoch 25/25, Loss: 0.00016345877635909982\nValidation Loss: 0.09787394535273512\nValidation Accuracy: 0.9789285714285715\n","output_type":"stream"}]}]}