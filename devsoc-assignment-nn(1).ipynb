{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nclass Linear:\n    def __init__(self, in_features, out_features):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.biases = np.zeros((1, out_features))\n        self.input = None\n        self.grad_weights = None\n        self.grad_biases = None\n\n    def forward(self, x):\n        self.input = x\n        return np.dot(x, self.weights) + self.biases\n\n    def backward(self, d_out):\n        self.grad_weights = np.dot(self.input.T, d_out)\n        self.grad_biases = np.sum(d_out, axis=0, keepdims=True)\n        d_input = np.dot(d_out, self.weights.T)\n        return d_input\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-11T18:47:45.987211Z","iopub.execute_input":"2024-08-11T18:47:45.988046Z","iopub.status.idle":"2024-08-11T18:47:46.000363Z","shell.execute_reply.started":"2024-08-11T18:47:45.988002Z","shell.execute_reply":"2024-08-11T18:47:45.998928Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"class ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n\n    def backward(self, d_out):\n        d_input = d_out.copy()\n        d_input[self.input <= 0] = 0\n        return d_input\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.003208Z","iopub.execute_input":"2024-08-11T18:47:46.003772Z","iopub.status.idle":"2024-08-11T18:47:46.020111Z","shell.execute_reply.started":"2024-08-11T18:47:46.003725Z","shell.execute_reply":"2024-08-11T18:47:46.018551Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"class Sigmoid:\n    def forward(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (self.output * (1 - self.output))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.022066Z","iopub.execute_input":"2024-08-11T18:47:46.022501Z","iopub.status.idle":"2024-08-11T18:47:46.036524Z","shell.execute_reply.started":"2024-08-11T18:47:46.022465Z","shell.execute_reply":"2024-08-11T18:47:46.034960Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"class Tanh:\n    def forward(self, x):\n        self.output = np.tanh(x)\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (1 - self.output ** 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.039183Z","iopub.execute_input":"2024-08-11T18:47:46.039623Z","iopub.status.idle":"2024-08-11T18:47:46.052996Z","shell.execute_reply.started":"2024-08-11T18:47:46.039578Z","shell.execute_reply":"2024-08-11T18:47:46.051615Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"class Softmax:\n    def forward(self, x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_out):\n        # Assuming d_out is already the gradient of loss w.r.t. softmax output\n        return d_out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.054692Z","iopub.execute_input":"2024-08-11T18:47:46.055247Z","iopub.status.idle":"2024-08-11T18:47:46.067298Z","shell.execute_reply.started":"2024-08-11T18:47:46.055205Z","shell.execute_reply":"2024-08-11T18:47:46.065804Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"class CrossEntropyLoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        self.n = y_true.shape[0]\n        loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / self.n\n        return loss\n\n    def backward(self):\n        return (self.y_pred - self.y_true) / self.y_true.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.069077Z","iopub.execute_input":"2024-08-11T18:47:46.069539Z","iopub.status.idle":"2024-08-11T18:47:46.081044Z","shell.execute_reply.started":"2024-08-11T18:47:46.069490Z","shell.execute_reply":"2024-08-11T18:47:46.079548Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"class MSELoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        return np.mean((y_pred - y_true) ** 2)\n\n    def backward(self):\n        return 2 * (self.y_pred - self.y_true) / self.y_true.size\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.083208Z","iopub.execute_input":"2024-08-11T18:47:46.083710Z","iopub.status.idle":"2024-08-11T18:47:46.094056Z","shell.execute_reply.started":"2024-08-11T18:47:46.083666Z","shell.execute_reply":"2024-08-11T18:47:46.092481Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"class SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        if hasattr(layer, 'grad_weights'):\n            layer.weights -= self.learning_rate * layer.grad_weights\n            layer.biases -= self.learning_rate * layer.grad_biases\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.106558Z","iopub.execute_input":"2024-08-11T18:47:46.108328Z","iopub.status.idle":"2024-08-11T18:47:46.116404Z","shell.execute_reply.started":"2024-08-11T18:47:46.108286Z","shell.execute_reply":"2024-08-11T18:47:46.114879Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_fn = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self, loss_fn, optimizer):\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n\n    def backward(self, loss_grad):\n        for layer in reversed(self.layers):\n            loss_grad = layer.backward(loss_grad)\n\n    def train(self, x_train, y_train, epochs, batch_size):\n        for epoch in range(epochs):\n            # Shuffle training data\n            indices = np.arange(x_train.shape[0])\n            np.random.shuffle(indices)\n            x_train, y_train = x_train[indices], y_train[indices]\n            \n            # Mini-batch training\n            for start in range(0, x_train.shape[0], batch_size):\n                end = min(start + batch_size, x_train.shape[0])\n                x_batch, y_batch = x_train[start:end], y_train[start:end]\n                \n                # Forward pass\n                predictions = self.forward(x_batch)\n                \n                # Compute loss\n                loss = self.loss_fn.forward(predictions, y_batch)\n                \n                # Backward pass\n                loss_grad = self.loss_fn.backward()\n                self.backward(loss_grad)\n                \n                # Update parameters\n                for layer in self.layers:\n                    self.optimizer.step(layer)\n                \n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n            self.display_weights()\n\n    def display_weights(self):\n        for i, layer in enumerate(self.layers):\n            if hasattr(layer, 'weights'):\n                print(f'Layer {i + 1} weights:\\n{layer.weights}')\n            if hasattr(layer, 'biases'):\n                print(f'Layer {i + 1} biases:\\n{layer.biases}')\n\n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss_fn.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n\n    def save(self, filename):\n        # Implement model saving\n        pass\n\n    def load(self, filename):\n        # Implement model loading\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.120967Z","iopub.execute_input":"2024-08-11T18:47:46.121966Z","iopub.status.idle":"2024-08-11T18:47:46.146664Z","shell.execute_reply.started":"2024-08-11T18:47:46.121869Z","shell.execute_reply":"2024-08-11T18:47:46.145413Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Separate features and labels\nx = train_df.drop(columns='label').values\ny = train_df['label'].values\n\n# Normalize pixel values\nx = x / 255.0\n\n# Reshape data to (num_samples, 28, 28)\nx = x.reshape(-1, 28, 28)\n\n# One-hot encode the labels\ny = to_categorical(y, 10)\n\n# Flatten the data\nx = x.reshape(x.shape[0], -1)\n\n# Split data into training and validation sets\n\nnp.random.seed(42)  # For reproducibility\nindices = np.arange(x.shape[0])\nnp.random.shuffle(indices)\nsplit_index = int(0.8 * len(indices))\n\ntrain_indices = indices[:split_index]\nval_indices = indices[split_index:]\n\nx_train, x_val = x[train_indices], x[val_indices]\ny_train, y_val = y[train_indices], y[val_indices]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:46.148019Z","iopub.execute_input":"2024-08-11T18:47:46.148598Z","iopub.status.idle":"2024-08-11T18:47:50.516513Z","shell.execute_reply.started":"2024-08-11T18:47:46.148547Z","shell.execute_reply":"2024-08-11T18:47:50.515169Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Initialize model and layers\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\n# Compile model with loss and optimizer\nloss_fn = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.5)\nmodel.compile(loss_fn, optimizer)\n\n# Train the model\nmodel.train(x_train, y_train, epochs=25, batch_size=64)\n\n# Evaluate the model on the validation dataset\nval_loss, val_accuracy = model.evaluate(x_val, y_val)\nprint(f'Validation Loss: {val_loss}')\nprint(f'Validation Accuracy: {val_accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:47:50.518040Z","iopub.execute_input":"2024-08-11T18:47:50.518398Z","iopub.status.idle":"2024-08-11T18:48:35.781018Z","shell.execute_reply.started":"2024-08-11T18:47:50.518366Z","shell.execute_reply":"2024-08-11T18:48:35.779460Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Epoch 1/25, Loss: 0.21772892976406097\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-0.09167512  0.055377    0.03285778 -0.00265917 -0.01227313  0.01082208\n   0.05030709 -0.00405591  0.01364748 -0.08419697 -0.07049043  0.10077101\n   0.08854419  0.03190927  0.15974093 -0.18691112 -0.04307237 -0.12581117\n   0.02811582 -0.045116   -0.09178836  0.04227565 -0.00206268 -0.04578999\n   0.04258042 -0.01487352 -0.09543641 -0.20361982 -0.11643094  0.04941899\n  -0.01081808 -0.02011108 -0.0359043   0.05532356  0.08886488  0.00148627\n   0.08986383  0.02432792  0.03614925 -0.06914412  0.12884402  0.02489856\n   0.0654161  -0.00475658 -0.04607589 -0.0116924   0.06366865  0.12467205\n  -0.04192485 -0.04759678 -0.03476235  0.06093455 -0.04628929  0.03604212\n  -0.06895688  0.04546433 -0.10355414  0.03093142  0.01506281 -0.13641985\n  -0.137325    0.00301206 -0.01927436  0.00305953 -0.00859318 -0.04766613\n   0.16032825 -0.09069033  0.01257718 -0.11697685 -0.01090319 -0.01426846\n  -0.00257135  0.16290647  0.11859735  0.02652538 -0.1338583  -0.06150511\n   0.02549219  0.11015243  0.01842221  0.03688308  0.02830339  0.08184344\n   0.08125519  0.0951294   0.06409885  0.1151832   0.05884549 -0.05884483\n  -0.09864352  0.04351332  0.01792127  0.00185652  0.06466839  0.26821699\n   0.11287469  0.02804646  0.05184663  0.07938339  0.02679927  0.00437445\n   0.07857989  0.05004607 -0.05043407  0.02912304 -0.03375605  0.09258926\n   0.02630734 -0.12579003  0.01766348  0.09572911  0.03368116  0.02740202\n   0.00128691  0.03066446  0.06534693  0.04915597  0.10529743  0.0313571\n  -0.0732673   0.03437676 -0.05637447  0.0720842   0.08146349 -0.02981815\n   0.01106392 -0.05253395]]\nLayer 3 weights:\n[[ 0.05930729 -0.0540869   0.14715468 ... -0.27949695  0.2144463\n  -0.08699323]\n [-0.15058173 -0.21914147 -0.27724672 ...  0.11418761 -0.17898224\n   0.40379596]\n [-0.0256031  -0.13901976  0.35147431 ... -0.18979426 -0.18283027\n   0.00148124]\n ...\n [-0.02280568 -0.03407062 -0.04837537 ... -0.01182761  0.12482894\n  -0.08960375]\n [ 0.27139809 -0.2724464   0.03549893 ... -0.21840457  0.34955892\n  -0.341404  ]\n [-0.07298362 -0.13574728  0.02777266 ...  0.03235963  0.11796739\n   0.15340458]]\nLayer 3 biases:\n[[-0.23518076  0.15850029 -0.1886723  -0.06999137  0.1848962   0.34313633\n  -0.07536478  0.13251382 -0.19199089 -0.05784656]]\nEpoch 2/25, Loss: 0.11674677436346408\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.24872277e-01  6.99107710e-02  4.92543289e-02 -2.44156588e-03\n  -1.93319489e-02  1.91376104e-02  3.70019777e-02 -4.06387364e-03\n   1.36436327e-02 -1.00650465e-01 -8.37002086e-02  1.04921445e-01\n   1.08010638e-01  2.49293512e-02  1.73545025e-01 -2.16045621e-01\n  -5.38327409e-02 -1.24693591e-01  3.49296381e-02 -4.75695263e-02\n  -1.20333306e-01  5.08743272e-02 -1.49997706e-03 -7.90751061e-02\n   4.44952701e-02 -2.16455248e-02 -1.03726738e-01 -2.54245770e-01\n  -1.44821719e-01  8.36627708e-02 -1.60323112e-02 -1.04172951e-02\n  -4.39171499e-02  4.32755237e-02  9.47016834e-02  4.64101289e-03\n   1.42084811e-01  1.61447209e-02  9.22451823e-05 -6.81401713e-02\n   1.39105036e-01  2.60469703e-02  7.49848243e-02 -4.87787198e-03\n  -5.25198796e-02 -2.16115760e-02  8.01060799e-02  1.19365548e-01\n  -9.65544314e-02 -6.42813704e-02 -1.52954190e-02  3.48554581e-02\n  -4.24076474e-02  4.52104979e-02 -9.20879838e-02  6.56112912e-02\n  -1.01563079e-01  5.74591384e-02  8.46713104e-03 -1.41250583e-01\n  -1.58990690e-01  3.33432268e-03 -5.21452255e-02  7.13379110e-03\n   1.73591742e-02 -5.84813056e-02  1.44832366e-01 -1.08120864e-01\n   8.66685568e-03 -1.38750566e-01 -3.05656605e-02 -2.66614799e-02\n  -2.28980032e-03  1.62383810e-01  1.45220820e-01  4.64522591e-02\n  -1.35129363e-01 -7.02016445e-02  2.55438456e-02  1.42296866e-01\n   4.68971537e-02  2.86293621e-02 -7.32845305e-03  9.10400161e-02\n   1.14779928e-01  9.92685345e-02  6.44795697e-02  1.29313948e-01\n   8.36143198e-02 -7.94464478e-02 -1.09214515e-01  5.65293934e-02\n  -1.10913977e-03  1.60589695e-03  7.72784614e-02  3.32209876e-01\n   1.49481263e-01  1.54317386e-02  5.32612809e-02  9.06109312e-02\n   4.75043332e-02 -1.97793181e-02  8.39138708e-02  3.60732469e-02\n  -5.29249184e-02  5.45436014e-02 -4.05512582e-02  1.10830740e-01\n  -3.35204716e-02 -1.76499453e-01  2.68302401e-02  1.22422197e-01\n   4.05808003e-02  3.44644338e-02  3.65301665e-02  4.26405003e-02\n   8.56937270e-02  6.27574997e-02  1.58347118e-01  3.35125207e-02\n  -8.22204662e-02  2.10964118e-02 -2.81322936e-02  9.86762985e-02\n   8.91466296e-02 -2.29316280e-02  1.16241372e-02 -7.75478169e-02]]\nLayer 3 weights:\n[[ 0.06902921 -0.02433808  0.18498191 ... -0.33069229  0.23997602\n  -0.09933779]\n [-0.15775048 -0.23796069 -0.40219158 ...  0.12940771 -0.15369562\n   0.50796701]\n [-0.06437252 -0.09391012  0.38679537 ... -0.20361644 -0.19792641\n  -0.00845174]\n ...\n [-0.02690775 -0.01193392 -0.07426688 ...  0.01130911  0.11699342\n  -0.15074464]\n [ 0.315227   -0.25611877  0.05830116 ... -0.25858308  0.39138856\n  -0.53201537]\n [-0.05525867 -0.14487202 -0.04679898 ...  0.06451449  0.13725389\n   0.16826216]]\nLayer 3 biases:\n[[-0.26332025  0.15455051 -0.23563733 -0.1573502   0.24205917  0.32613486\n  -0.08379243  0.0668077  -0.03919608 -0.01025595]]\nEpoch 3/25, Loss: 0.25393391405482685\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.17998065e-01  4.00411763e-02  5.72523442e-02 -9.59238769e-04\n  -2.15245454e-02  3.70494768e-02  2.55905975e-02 -3.69659591e-03\n   7.46807778e-03 -9.80206839e-02 -8.58417265e-02  1.02042033e-01\n   1.36126954e-01  1.03889775e-02  1.81120298e-01 -2.29304487e-01\n  -5.94729223e-02 -1.30434371e-01  3.80458691e-02 -4.72644746e-02\n  -1.42160635e-01  4.88217373e-02 -9.14705797e-04 -9.01674689e-02\n   4.25118413e-02 -1.89577418e-02 -1.05210466e-01 -3.12366208e-01\n  -1.37457663e-01  1.07586975e-01 -1.73135563e-02  2.53341867e-03\n  -4.31932528e-03  4.26351141e-02  8.66414500e-02  3.16995226e-03\n   1.64514910e-01  2.54062098e-02 -1.09652727e-02 -6.44746881e-02\n   1.73305620e-01  1.83375229e-02  8.20201759e-02 -4.93913595e-03\n  -5.34921862e-02 -3.03071573e-02  8.30603982e-02  1.13610599e-01\n  -1.15170962e-01 -6.02439188e-02 -1.60676360e-02  1.14191899e-02\n  -3.81713574e-02  7.76750971e-03 -1.14079214e-01  8.08024853e-02\n  -1.04816712e-01  1.02215039e-01  2.06315981e-02 -1.70471124e-01\n  -1.63188809e-01  4.75647908e-03 -7.66025534e-02  5.86788623e-03\n   4.02700484e-02 -6.71227344e-02  1.57474712e-01 -1.02601231e-01\n  -3.21203940e-04 -1.39243635e-01 -2.79763527e-02 -3.39958891e-02\n  -1.82995937e-03  1.50321422e-01  1.43226151e-01  6.64968794e-02\n  -1.18451477e-01 -6.81848336e-02  4.26169941e-02  1.67362327e-01\n   5.59326369e-02  3.28717736e-02 -2.75203745e-02  1.21615025e-01\n   1.58104466e-01  1.33447581e-01  1.43612082e-02  1.35303366e-01\n   1.02325299e-01 -5.36591707e-02 -1.04186552e-01  6.51477152e-02\n  -2.93392023e-02  4.36080188e-03  9.45169076e-02  3.52985125e-01\n   1.76307331e-01  9.88574388e-03  8.50643073e-02  8.06926122e-02\n   4.21793108e-02 -2.21021607e-02  1.09332618e-01  2.49529582e-02\n  -5.60928699e-02  6.02842600e-02 -3.52204579e-02  9.70454462e-02\n  -4.31227693e-02 -1.88968351e-01  3.99364219e-02  1.42622054e-01\n   3.30628845e-02  3.69181673e-02  5.19837701e-02  7.01897922e-02\n   1.02522396e-01  8.47451547e-02  1.71475582e-01  4.64499896e-02\n  -1.01350730e-01  3.73627323e-02 -2.64383219e-02  8.93576432e-02\n   6.85000195e-02 -2.80681643e-02  4.59870291e-02 -6.49938013e-02]]\nLayer 3 weights:\n[[ 0.0457685   0.00958812  0.23777622 ... -0.38905404  0.2798124\n  -0.07843109]\n [-0.14209567 -0.2473994  -0.4187532  ...  0.09602626 -0.09794689\n   0.53695019]\n [-0.11105817 -0.06592044  0.4145203  ... -0.19813226 -0.22145754\n  -0.01658296]\n ...\n [-0.04554645  0.03205002 -0.09035344 ...  0.04136525  0.11422847\n  -0.21450593]\n [ 0.28836034 -0.23191077  0.06287902 ... -0.23897231  0.50896088\n  -0.69413617]\n [-0.04643187 -0.107837   -0.04440931 ...  0.05436793  0.13234763\n   0.21120767]]\nLayer 3 biases:\n[[-0.30856833  0.15616025 -0.24139486 -0.2095927   0.27510744  0.30851792\n  -0.09274992  0.03087009  0.10033405 -0.01868394]]\nEpoch 4/25, Loss: 0.011720801745656938\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.23420550e-01  4.78694855e-02  6.01214840e-02  2.47049035e-04\n  -1.82887314e-02  1.38754361e-02 -3.30399406e-03 -3.47082592e-03\n   3.46181926e-03 -1.25272791e-01 -1.10253263e-01  1.04886967e-01\n   1.46777423e-01  1.12489365e-02  2.04107908e-01 -2.20610442e-01\n  -7.57377529e-02 -1.26888802e-01  3.94129449e-02 -4.85438647e-02\n  -1.46621372e-01  4.06092807e-02  2.88240435e-05 -9.74297656e-02\n   3.81614143e-02 -2.30123272e-02 -1.09761725e-01 -3.25255404e-01\n  -1.31576623e-01  1.31305574e-01 -2.52844585e-02  3.90652654e-03\n   8.41116802e-03  5.76421116e-02  1.08480427e-01  1.24643772e-03\n   1.95678619e-01  2.75217039e-02 -2.15542155e-02 -6.12900294e-02\n   1.95934554e-01  2.21993834e-02  9.41863378e-02 -4.97863640e-03\n  -5.36803715e-02 -3.63802541e-02  7.61692708e-02  9.15773101e-02\n  -1.50568414e-01 -3.74407077e-02 -2.58787097e-02  4.58606611e-03\n  -2.48759640e-02  2.04605953e-02 -1.13804174e-01  9.78482009e-02\n  -1.08667284e-01  1.16222618e-01  5.66260680e-03 -1.68722728e-01\n  -1.52006965e-01  3.45517407e-03 -8.95535475e-02  7.57676910e-03\n   4.54632945e-02 -7.24110387e-02  1.39194922e-01 -1.05137548e-01\n  -1.64272048e-03 -1.52768585e-01 -3.89365666e-02 -3.68587774e-02\n  -1.60334025e-03  1.30965774e-01  1.56737949e-01  8.15150025e-02\n  -1.04514672e-01 -8.11028753e-02  3.42277325e-02  2.08187965e-01\n   6.66160819e-02  1.72388856e-02 -5.67911562e-02  1.33678168e-01\n   1.61008688e-01  1.17808698e-01 -9.15728914e-03  1.49021914e-01\n   1.10005913e-01 -3.66852128e-02 -1.10711200e-01  7.01156047e-02\n  -4.78321554e-02 -6.27511988e-04  1.12284289e-01  3.45685062e-01\n   1.94512624e-01 -9.85459862e-03  7.93778067e-02  8.64780219e-02\n   4.70343656e-02 -3.51024794e-02  1.01801554e-01  1.34680682e-02\n  -5.59412416e-02  6.37092926e-02 -4.31574716e-02  1.11602300e-01\n  -5.38567645e-02 -1.98565764e-01  6.42046274e-02  1.28525586e-01\n   3.78234584e-02  5.13345611e-02  5.92953560e-02  8.71907495e-02\n   1.04194515e-01  9.74412273e-02  1.98244472e-01  3.83925467e-02\n  -1.35159372e-01  2.73959813e-02 -3.38304890e-03  9.86881146e-02\n   7.72210822e-02 -1.98330670e-02  4.85753393e-02 -4.99152209e-02]]\nLayer 3 weights:\n[[ 0.06381302  0.09114298  0.20266572 ... -0.45877862  0.30436426\n  -0.05556568]\n [-0.14958215 -0.25026441 -0.5106795  ...  0.1322504  -0.09984893\n   0.58224134]\n [-0.11854295 -0.0548532   0.45288905 ... -0.21821888 -0.23623185\n  -0.03189566]\n ...\n [-0.03802758  0.0680319  -0.10980019 ...  0.06329261  0.09024848\n  -0.26461875]\n [ 0.33901669 -0.17493104  0.11095215 ... -0.25284811  0.48480051\n  -0.79940168]\n [-0.03122543 -0.15009295 -0.05362703 ...  0.02257578  0.22930072\n   0.2360405 ]]\nLayer 3 biases:\n[[-0.31227677  0.14638506 -0.25108043 -0.22499519  0.30534758  0.29042713\n  -0.11854404 -0.02405249  0.1853556   0.00343357]]\nEpoch 5/25, Loss: 0.10206746395636827\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.42744449e-01  3.68090606e-02  5.21943117e-02  3.01619676e-04\n  -3.17420953e-02  2.29259287e-02 -3.35608615e-03 -3.28249836e-03\n  -7.79355897e-03 -1.12038722e-01 -1.27982261e-01  8.45942390e-02\n   1.43550868e-01  5.05408262e-03  2.12271534e-01 -2.34895742e-01\n  -9.21150268e-02 -1.14834825e-01  3.30178018e-02 -4.33756540e-02\n  -1.53133449e-01  4.15904505e-02 -7.98633115e-04 -1.20296733e-01\n   3.28831634e-02 -2.64108221e-02 -1.07496026e-01 -3.32565394e-01\n  -1.08080377e-01  1.39214208e-01 -2.73551263e-02  5.04017169e-02\n   1.70776861e-02  4.96182497e-02  9.81092872e-02 -1.32538213e-03\n   1.90617543e-01  3.29240900e-02 -2.21268667e-02 -7.84629278e-02\n   1.99861866e-01  3.36374060e-02  8.90168403e-02 -4.74565376e-03\n  -7.82033037e-02 -4.05582856e-02  7.47822072e-02  8.99172125e-02\n  -1.50982524e-01 -4.49322438e-02 -1.03283936e-02 -2.11473225e-02\n  -2.32225184e-02 -8.63133564e-03 -1.10659744e-01  1.10769110e-01\n  -9.50762917e-02  1.51261231e-01  2.88704715e-02 -1.87290331e-01\n  -1.62948867e-01  1.72618077e-03 -1.25156543e-01  3.05253024e-03\n   4.90782769e-02 -8.12159925e-02  1.33673909e-01 -9.79892733e-02\n  -6.13414644e-03 -1.68314791e-01 -2.85029956e-02 -4.14838415e-02\n  -1.20888186e-03  1.26419738e-01  1.54610334e-01  9.19651466e-02\n  -9.72307067e-02 -8.27867400e-02  4.48108845e-02  2.04645015e-01\n   8.41495030e-02  4.59005487e-03 -5.65283603e-02  1.38997153e-01\n   1.67666031e-01  1.27143933e-01  6.50271050e-03  1.49002184e-01\n   1.32307846e-01 -8.50386266e-03 -1.08495317e-01  5.31384123e-02\n  -5.25601242e-02 -7.65759582e-03  1.10508750e-01  3.59785324e-01\n   1.91136717e-01 -1.42323790e-02  9.77549591e-02  6.21627713e-02\n   5.54575168e-02 -3.94564900e-02  1.17039130e-01  2.62961757e-02\n  -5.35832758e-02  8.27153257e-02 -8.72717014e-03  8.46437167e-02\n  -4.66465360e-02 -2.07113627e-01  7.29484304e-02  1.55406348e-01\n   2.96121871e-02  4.67071689e-02  8.56471237e-02  8.92559453e-02\n   1.30007601e-01  9.86729868e-02  2.50291487e-01  2.70132031e-02\n  -1.54119095e-01  3.82968687e-02 -1.75332300e-03  1.17940577e-01\n   7.94171762e-02 -3.57978421e-02  6.70542724e-02 -5.19267835e-02]]\nLayer 3 weights:\n[[ 0.07462305  0.11451476  0.19460189 ... -0.50054288  0.29501436\n  -0.06306225]\n [-0.14042788 -0.25533601 -0.5004463  ...  0.04428664 -0.0672701\n   0.64761594]\n [-0.14820715 -0.06755478  0.48372778 ... -0.20570655 -0.26018333\n  -0.03615252]\n ...\n [-0.042951    0.10145519 -0.15802291 ...  0.09486284  0.10930069\n  -0.30299485]\n [ 0.36388359 -0.16843102  0.10069084 ... -0.21407583  0.49560873\n  -0.85705502]\n [-0.02822635 -0.14302634 -0.03345929 ...  0.03940561  0.19002064\n   0.22518745]]\nLayer 3 biases:\n[[-0.33414214  0.13054165 -0.25727125 -0.21099879  0.31259358  0.27483083\n  -0.10133283 -0.05281486  0.2224167   0.01617711]]\nEpoch 6/25, Loss: 0.08921165988671861\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-0.14574652  0.04941405  0.06244518  0.00153916 -0.03700036  0.0347636\n  -0.01866998 -0.00295524 -0.00282842 -0.15198235 -0.15475789  0.0876979\n   0.16257087  0.00503736  0.21341985 -0.23262376 -0.09795846 -0.11306183\n   0.02871096 -0.04035722 -0.15262769  0.04676112 -0.00052647 -0.13040134\n   0.02350544 -0.02424544 -0.11586828 -0.33317819 -0.11264407  0.15387473\n  -0.02946153  0.01941741  0.05432825  0.05939678  0.10442641  0.00809637\n   0.19899372  0.04225115 -0.04357365 -0.06237631  0.20800503  0.03065655\n   0.08863215 -0.00513475 -0.07815486 -0.04362575  0.08627502  0.06386245\n  -0.17095321 -0.02680231 -0.01079036 -0.04899413 -0.01566987 -0.00843771\n  -0.09140483  0.09924394 -0.10078966  0.17683823  0.01765085 -0.19193427\n  -0.15872448  0.00306446 -0.12210909  0.00230345  0.04197482 -0.091148\n   0.15742615 -0.10173768 -0.00296326 -0.16146982 -0.04485547 -0.04347952\n  -0.00113059  0.1190897   0.1617473   0.08906343 -0.07237283 -0.07349115\n   0.04545179  0.20810486  0.07347837  0.02349816 -0.08564509  0.15703864\n   0.16644551  0.13335749 -0.0283441   0.14422073  0.12257402  0.01591147\n  -0.10215601  0.04762207 -0.04380676 -0.01568217  0.10157679  0.35435866\n   0.21567459 -0.03383874  0.10145814  0.07072015  0.05819308 -0.06313723\n   0.1387832   0.03070058 -0.03302224  0.09057684 -0.002139    0.13285792\n  -0.03644446 -0.20864739  0.08766476  0.1133238   0.02309679  0.06055936\n   0.07747218  0.09613939  0.12912543  0.11356753  0.25648126 -0.00121121\n  -0.17405834  0.0439659   0.03862173  0.13096563  0.05985308 -0.01716395\n   0.07394147 -0.02629864]]\nLayer 3 weights:\n[[ 0.06952387  0.15724444  0.21775971 ... -0.55289572  0.29850297\n  -0.03458767]\n [-0.12104711 -0.25724598 -0.55969973 ...  0.08205628 -0.04866192\n   0.63409777]\n [-0.17875567 -0.05884578  0.53332643 ... -0.2240734  -0.27247594\n  -0.03956075]\n ...\n [-0.04322698  0.14471192 -0.19674025 ...  0.13824705  0.09766512\n  -0.34972757]\n [ 0.33552712 -0.11886258  0.16135793 ... -0.25560748  0.49467744\n  -0.96014999]\n [-0.02694328 -0.15024037  0.0152442  ...  0.00763536  0.24189424\n   0.22601131]]\nLayer 3 biases:\n[[-0.36218246  0.14290171 -0.25560012 -0.2435495   0.33580223  0.2823564\n  -0.13099407 -0.06956645  0.28730266  0.01352958]]\nEpoch 7/25, Loss: 0.017043194145259163\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-0.13959334  0.0609755   0.07229707  0.00169836 -0.03903521  0.04315118\n  -0.03927416 -0.00292205 -0.00853833 -0.13915427 -0.16586595  0.06883253\n   0.18472787 -0.00322705  0.21475396 -0.2179537  -0.09477051 -0.11165888\n   0.03833078 -0.03567907 -0.14553864  0.05893097 -0.00037871 -0.13678001\n   0.00352121 -0.02783359 -0.10723988 -0.32699638 -0.08914538  0.19668844\n  -0.04708798  0.03350071  0.0712691   0.0600066   0.12098402  0.0080294\n   0.21155445  0.04593981 -0.03340391 -0.06573309  0.21610154  0.04377644\n   0.08251831 -0.00508531 -0.09128083 -0.04466497  0.0770208   0.05327433\n  -0.18657263 -0.02049673 -0.00946787 -0.06013369 -0.01579205  0.0094551\n  -0.09327028  0.11979546 -0.08737156  0.19583231  0.02543885 -0.19496694\n  -0.16908849  0.00293413 -0.14543539  0.00309969  0.04486447 -0.09900866\n   0.15294565 -0.10289161 -0.01347707 -0.17339442 -0.0174849  -0.04451317\n  -0.00082127  0.12812557  0.17910268  0.10920619 -0.0795915  -0.06787559\n   0.03807988  0.2219216   0.09015902  0.00573405 -0.08510109  0.17715611\n   0.17987302  0.16386483 -0.0363532   0.14800289  0.12716297  0.01172476\n  -0.10777865  0.04525596 -0.06368834 -0.02044052  0.11674734  0.3506102\n   0.21683531 -0.04657326  0.10444087  0.04334186  0.05090327 -0.07511182\n   0.13489236  0.03471579 -0.0233038   0.09274878  0.01650331  0.15995347\n  -0.0238348  -0.21505021  0.07105035  0.13689049  0.01521075  0.06117987\n   0.0852545   0.09932378  0.14076495  0.12556175  0.26577449 -0.00110957\n  -0.2014811   0.04704412  0.0540497   0.13609541  0.04860707 -0.02756696\n   0.05937486 -0.01746775]]\nLayer 3 weights:\n[[ 0.10848564  0.1812901   0.23255922 ... -0.61575058  0.31545167\n  -0.02551126]\n [-0.12554081 -0.2508267  -0.57550659 ...  0.06763204 -0.02421555\n   0.60825262]\n [-0.19113494 -0.06684351  0.54829601 ... -0.22070929 -0.29764439\n  -0.0399656 ]\n ...\n [-0.03569122  0.17212529 -0.22398761 ...  0.1511482   0.09781094\n  -0.37859248]\n [ 0.43516279 -0.0820326   0.16942887 ... -0.2725156   0.50455365\n  -0.98277624]\n [-0.00570676 -0.18323652 -0.00553453 ...  0.02381713  0.25763873\n   0.28004971]]\nLayer 3 biases:\n[[-0.36306808  0.12966709 -0.2315351  -0.27399418  0.33469988  0.28359081\n  -0.13660266 -0.09871305  0.32073936  0.03521593]]\nEpoch 8/25, Loss: 0.012681621194170118\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-0.13995612  0.05632437  0.07336988  0.00223121 -0.03255214  0.03944262\n  -0.04181584 -0.00313651 -0.00937812 -0.13930138 -0.18099951  0.06721791\n   0.19488068  0.00420655  0.22448441 -0.19958853 -0.10643591 -0.12313171\n   0.02668828 -0.03223763 -0.14928015  0.05894655 -0.0005691  -0.14536906\n   0.00981998 -0.02357143 -0.10848609 -0.31738255 -0.09087953  0.22559637\n  -0.02686539  0.04858494  0.06676138  0.05604485  0.11279612  0.00651801\n   0.21459731  0.05002269 -0.05728714 -0.05086505  0.20801713  0.04390472\n   0.06953783 -0.00509426 -0.09244741 -0.04675746  0.08769455  0.047298\n  -0.20610912 -0.03504914 -0.00061286 -0.06694124 -0.00787623  0.02144345\n  -0.09778782  0.13588434 -0.07795417  0.19807727  0.03096324 -0.2245146\n  -0.16778606  0.00117962 -0.15196037  0.00440947  0.07002389 -0.09837163\n   0.15343893 -0.09955025 -0.01409485 -0.16490634 -0.01495688 -0.04209856\n  -0.00103963  0.1110365   0.15523169  0.11306005 -0.08932985 -0.06216458\n   0.04143096  0.22890503  0.08741782  0.00776462 -0.10582189  0.19018274\n   0.16520991  0.15813215 -0.04325664  0.15197815  0.14287691  0.03188518\n  -0.10580594  0.03042819 -0.07474437 -0.02165558  0.11533222  0.32800993\n   0.20891214 -0.03179265  0.12011565  0.02849537  0.05144206 -0.07331514\n   0.13948434  0.03346615 -0.06165122  0.09480366  0.01390596  0.1585882\n  -0.01519298 -0.22150695  0.09057067  0.1367088   0.00819155  0.09450958\n   0.08917195  0.10109884  0.14069278  0.14282797  0.27972208 -0.01860195\n  -0.211276    0.06152694  0.06401339  0.1368682   0.03166401 -0.03514032\n   0.09337525 -0.00948413]]\nLayer 3 weights:\n[[ 1.21950833e-01  2.09878719e-01  2.27329987e-01 ... -6.63291688e-01\n   2.84375019e-01  2.78631693e-03]\n [-1.13499383e-01 -2.52676272e-01 -6.22017667e-01 ...  3.62724149e-02\n  -1.62844100e-02  6.64418314e-01]\n [-2.04953248e-01 -5.82776739e-02  5.57130354e-01 ... -2.21810324e-01\n  -2.97555094e-01 -4.01531384e-02]\n ...\n [-3.84941679e-02  2.09900717e-01 -2.38892631e-01 ...  1.68243109e-01\n   1.16653940e-01 -4.29971028e-01]\n [ 3.91773660e-01 -7.83723027e-02  1.74308639e-01 ... -2.74457933e-01\n   5.08621433e-01 -1.01589726e+00]\n [-8.39106243e-04 -1.68252954e-01  9.41181058e-03 ...  1.45212648e-03\n   2.26662307e-01  2.85276410e-01]]\nLayer 3 biases:\n[[-0.37140726  0.14348468 -0.22247965 -0.27380381  0.33545219  0.28070393\n  -0.14239341 -0.12181126  0.33823791  0.03401668]]\nEpoch 9/25, Loss: 0.009078825226183446\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.28476359e-01  7.19513701e-02  6.34811441e-02  2.56349154e-03\n  -3.62202579e-02  2.53295628e-02 -4.38522192e-02 -2.75590092e-03\n  -2.09944674e-02 -1.32036287e-01 -1.88098657e-01  6.25447537e-02\n   2.17405929e-01  1.00485415e-03  2.14714461e-01 -1.92891716e-01\n  -9.63453151e-02 -1.08531294e-01  1.42998008e-02 -2.13408887e-02\n  -1.44996937e-01  6.05350817e-02 -1.34701472e-04 -1.57501322e-01\n  -4.84670906e-03 -3.20972519e-02 -9.90539607e-02 -3.32735270e-01\n  -7.85165994e-02  2.18043701e-01 -3.04237295e-02  5.60498212e-02\n   8.85187980e-02  6.06918610e-02  1.16815470e-01  1.38458021e-02\n   2.20260621e-01  5.37513125e-02 -3.30829296e-02 -6.25581226e-02\n   2.03917647e-01  5.03002183e-02  7.36172055e-02 -5.21008114e-03\n  -1.01033509e-01 -4.39552818e-02  9.12934327e-02  4.62973999e-02\n  -2.03493830e-01 -5.29350997e-02 -3.35045193e-03 -9.66599140e-02\n  -8.26702249e-03  1.30979749e-02 -8.91642008e-02  1.25298872e-01\n  -9.14140130e-02  2.17177863e-01  2.56833755e-02 -2.20135690e-01\n  -1.65603252e-01  3.52196326e-03 -1.52463423e-01  4.74563472e-03\n   8.56688380e-02 -1.01930151e-01  1.43256644e-01 -9.10849880e-02\n  -2.06352464e-02 -1.84077466e-01 -6.97015659e-03 -4.74113656e-02\n  -7.93656499e-04  1.24144877e-01  1.39804376e-01  1.20236526e-01\n  -7.37525930e-02 -6.90692281e-02  4.38186143e-02  2.16882799e-01\n   9.36091567e-02 -6.67328082e-03 -1.00933837e-01  1.83017404e-01\n   1.82019835e-01  1.72902050e-01 -4.70887560e-02  1.54320203e-01\n   1.33743766e-01  4.52586562e-02 -9.50640215e-02  2.17591052e-02\n  -8.79119129e-02 -2.61475223e-02  1.11923314e-01  3.28603126e-01\n   2.12310679e-01 -2.61937925e-02  1.28812600e-01  2.27013549e-02\n   4.78904851e-02 -6.62347838e-02  1.60125874e-01  4.00900261e-02\n  -6.51008953e-02  9.53520208e-02  9.64893626e-03  1.50712159e-01\n  -8.06912754e-03 -2.18408543e-01  9.98488224e-02  1.19527592e-01\n   2.81949026e-03  1.00461724e-01  1.02467112e-01  1.11225978e-01\n   1.48062757e-01  1.44415731e-01  3.03885536e-01 -4.42263236e-02\n  -2.31659548e-01  6.46888526e-02  6.24428302e-02  1.33498865e-01\n   2.93321871e-02 -3.43165939e-02  9.38279290e-02 -1.15081857e-02]]\nLayer 3 weights:\n[[ 1.49614680e-01  2.31759960e-01  2.62195364e-01 ... -6.85708161e-01\n   3.32491328e-01 -3.26514569e-02]\n [-1.12212838e-01 -2.51539576e-01 -6.59755424e-01 ...  4.95483457e-02\n  -1.12626967e-02  6.79955133e-01]\n [-2.19223222e-01 -6.47229832e-02  5.85360540e-01 ... -2.20545911e-01\n  -3.06324674e-01 -3.94549756e-02]\n ...\n [-2.99400581e-02  2.42234751e-01 -2.88848505e-01 ...  1.87763991e-01\n   1.20952568e-01 -4.62033529e-01]\n [ 4.30741960e-01 -5.72847316e-02  1.17529259e-01 ... -2.43088648e-01\n   5.01839219e-01 -1.08256946e+00]\n [ 2.13476406e-02 -1.93317486e-01  3.92150652e-02 ... -2.19345960e-04\n   2.33092412e-01  2.92808200e-01]]\nLayer 3 biases:\n[[-0.36569916  0.13113251 -0.23696149 -0.2958169   0.34547929  0.2779061\n  -0.14498377 -0.1308855   0.37478178  0.04504716]]\nEpoch 10/25, Loss: 0.012021268714607356\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.32670031e-01  6.70450964e-02  7.16049253e-02  3.02225058e-03\n  -3.32305958e-02  2.93974833e-02 -3.80408244e-02 -2.35506795e-03\n  -2.44783126e-02 -1.42060303e-01 -2.06877740e-01  5.70923420e-02\n   2.24826010e-01 -9.98541038e-03  2.24528559e-01 -1.71081688e-01\n  -8.96165268e-02 -1.17513910e-01  2.49943245e-02 -2.60613533e-02\n  -1.39795461e-01  7.54420233e-02  6.18957891e-05 -1.60657778e-01\n  -1.35255572e-02 -3.51598781e-02 -1.03137037e-01 -3.19488121e-01\n  -7.64359105e-02  2.28924615e-01 -4.19017742e-02  6.32793337e-02\n   8.63856292e-02  7.27035510e-02  1.08835275e-01  1.53609650e-02\n   2.20285547e-01  6.00514301e-02 -4.66014359e-02 -4.67740588e-02\n   1.91409019e-01  4.44694033e-02  7.76721532e-02 -5.23105491e-03\n  -1.10259885e-01 -4.36522935e-02  9.08163962e-02  2.57460200e-02\n  -2.13224118e-01 -3.43898691e-02 -1.57493726e-02 -9.77366504e-02\n  -1.32159596e-02  1.38448835e-02 -8.51833653e-02  1.38320570e-01\n  -8.19394766e-02  2.24032115e-01  3.20730385e-02 -2.42287187e-01\n  -1.61284027e-01  4.06074509e-03 -1.62925428e-01  6.78774798e-03\n   7.94865714e-02 -1.09604224e-01  1.37879711e-01 -9.28192523e-02\n  -1.75499809e-02 -1.84624456e-01  2.61532351e-03 -4.78922266e-02\n  -9.70604211e-04  1.13791061e-01  1.49962748e-01  1.18877795e-01\n  -6.72679662e-02 -6.39223889e-02  4.82480074e-02  2.27867133e-01\n   8.89875289e-02 -2.40146758e-02 -8.68300327e-02  1.91860885e-01\n   1.85320616e-01  1.64573084e-01 -7.12401392e-02  1.53344022e-01\n   1.33165925e-01  5.27650396e-02 -9.64693289e-02  1.31844292e-02\n  -8.13350322e-02 -2.37565254e-02  1.07292374e-01  3.26958339e-01\n   2.01053179e-01 -5.38510414e-02  1.41912524e-01  3.19860641e-03\n   5.93496907e-02 -5.89622324e-02  1.74581136e-01  3.50416583e-02\n  -7.04654093e-02  9.74054725e-02  1.14086984e-02  1.60949681e-01\n  -1.39503039e-02 -2.16088161e-01  1.08002387e-01  1.22232302e-01\n   1.60095287e-03  1.15292965e-01  1.07777625e-01  1.00820754e-01\n   1.31498944e-01  1.49622925e-01  3.30423494e-01 -6.28123337e-02\n  -2.38775033e-01  7.64144372e-02  7.53955285e-02  1.35211288e-01\n   3.04160528e-02 -3.06621472e-02  1.17145382e-01  3.68038603e-04]]\nLayer 3 weights:\n[[ 0.13408089  0.2747379   0.27534901 ... -0.72058823  0.33336626\n  -0.00742693]\n [-0.08477648 -0.25427136 -0.69393477 ...  0.03045995  0.01449402\n   0.64764965]\n [-0.2325879  -0.06791717  0.60546739 ... -0.22793501 -0.31301069\n  -0.03910222]\n ...\n [-0.02359201  0.24549311 -0.30748504 ...  0.21726483  0.16801418\n  -0.49160973]\n [ 0.40623135 -0.07972949  0.15487526 ... -0.24152829  0.61890579\n  -1.14327485]\n [ 0.05095125 -0.20479587  0.02140617 ...  0.00139863  0.24017617\n   0.27498821]]\nLayer 3 biases:\n[[-0.3761885   0.12894463 -0.23469309 -0.29808641  0.35424417  0.28310592\n  -0.15446106 -0.1398276   0.39804142  0.03892053]]\nEpoch 11/25, Loss: 0.008234270360459834\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.30114318e-01  7.13219826e-02  7.00325913e-02  3.89144026e-03\n  -2.90514783e-02  2.85852021e-02 -5.60794359e-02 -2.57373453e-03\n  -3.09190271e-02 -1.54202167e-01 -1.92755493e-01  7.17166904e-02\n   2.40344294e-01 -6.87477207e-03  2.57654814e-01 -1.71842420e-01\n  -8.96932698e-02 -1.05974723e-01  2.44373417e-02 -1.88140243e-02\n  -1.36874699e-01  6.76693178e-02 -2.46936620e-04 -1.72029298e-01\n  -2.48199319e-02 -3.15399150e-02 -1.05477248e-01 -3.10125381e-01\n  -6.10967785e-02  2.33903914e-01 -2.44897150e-02  6.60384683e-02\n   9.90001610e-02  6.84454366e-02  1.06936121e-01  1.58099403e-02\n   2.33623676e-01  6.30613701e-02 -4.05941348e-02 -6.22410340e-02\n   2.14994226e-01  6.20310079e-02  7.99728362e-02 -5.26490839e-03\n  -1.12685575e-01 -4.80496193e-02  9.84425049e-02  3.22193186e-02\n  -1.97080174e-01 -4.14471751e-02 -1.24410045e-02 -1.02886062e-01\n  -1.41897012e-02  2.02169168e-02 -9.09304056e-02  1.45025241e-01\n  -8.46115697e-02  2.28725655e-01  2.92198389e-02 -2.41069968e-01\n  -1.61240628e-01  4.92760321e-03 -1.59295035e-01  5.84737819e-03\n   8.05337162e-02 -1.23654366e-01  1.33828436e-01 -9.02758939e-02\n  -1.77839909e-02 -1.90400066e-01  5.10288226e-03 -5.23132335e-02\n  -8.87803677e-04  1.15482106e-01  1.58703495e-01  1.25281540e-01\n  -6.48342123e-02 -5.99521088e-02  4.32460181e-02  2.24454883e-01\n   9.61374153e-02 -2.25294503e-02 -9.31146461e-02  1.94164392e-01\n   1.85163724e-01  1.64279666e-01 -7.09054804e-02  1.56852384e-01\n   1.33151089e-01  6.60807801e-02 -9.15822317e-02  9.20265667e-03\n  -7.88211820e-02 -2.27735106e-02  1.22709280e-01  3.22166699e-01\n   2.25590116e-01 -5.32139124e-02  1.45088341e-01 -1.46307292e-02\n   4.64058876e-02 -6.10879947e-02  1.91667920e-01  3.38871642e-02\n  -5.10938318e-02  9.92469195e-02 -4.55932260e-04  1.54039460e-01\n   2.36846800e-03 -2.14111102e-01  9.99243186e-02  1.12180099e-01\n   5.74911369e-03  1.07875154e-01  9.95721389e-02  1.07806195e-01\n   1.44117399e-01  1.63758124e-01  3.26315259e-01 -6.87681792e-02\n  -2.32633027e-01  7.48354509e-02  9.54746688e-02  1.50127152e-01\n   3.45490513e-02 -3.50466708e-02  1.29577123e-01  4.62695287e-03]]\nLayer 3 weights:\n[[ 0.14500688  0.28457501  0.28434943 ... -0.74526262  0.32466523\n  -0.00253977]\n [-0.11033959 -0.25532844 -0.71106719 ...  0.07696872  0.01608227\n   0.655411  ]\n [-0.24072015 -0.06588051  0.6198426  ... -0.23062626 -0.32718842\n  -0.03693334]\n ...\n [-0.02001133  0.26713011 -0.32417082 ...  0.22847355  0.14847089\n  -0.51753067]\n [ 0.45570781 -0.07985516  0.13483751 ... -0.24525678  0.59119473\n  -1.17171955]\n [ 0.06350474 -0.23099257  0.04119088 ...  0.00577871  0.22766381\n   0.28853939]]\nLayer 3 biases:\n[[-0.36751667  0.12928754 -0.2307355  -0.31216299  0.34950157  0.29202399\n  -0.16004307 -0.1496214   0.40773446  0.04153207]]\nEpoch 12/25, Loss: 0.004146135683986552\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.45467827e-01  8.57894248e-02  6.76909230e-02  4.33106793e-03\n  -2.21379532e-02  1.70809947e-02 -6.36399481e-02 -2.70808302e-03\n  -3.30412284e-02 -1.45864585e-01 -1.91322889e-01  5.89332122e-02\n   2.40159824e-01 -1.49752206e-02  2.42437985e-01 -1.53422797e-01\n  -8.35915508e-02 -1.00310036e-01  9.68828822e-03 -1.83462272e-02\n  -1.37069110e-01  6.79391883e-02  2.47878881e-04 -1.72906891e-01\n  -1.83870132e-02 -2.98619381e-02 -9.87627439e-02 -3.11299748e-01\n  -5.52089330e-02  2.41309129e-01 -2.96810271e-02  8.47280589e-02\n   1.16269561e-01  6.19351763e-02  9.74975648e-02  1.82097034e-02\n   2.30326966e-01  6.70492985e-02 -4.94840554e-02 -6.51991851e-02\n   2.14691155e-01  5.87428640e-02  7.60762760e-02 -5.28464494e-03\n  -1.15476481e-01 -4.24447244e-02  9.56375754e-02  1.91364174e-02\n  -2.10246840e-01 -3.42629283e-02 -1.19183219e-02 -1.06804887e-01\n   2.64875705e-03  1.64588232e-02 -8.08044439e-02  1.35529291e-01\n  -8.11759174e-02  2.17153892e-01  3.08812208e-02 -2.52668727e-01\n  -1.59319811e-01  5.45276123e-03 -1.66324550e-01  2.99522618e-03\n   1.04431390e-01 -1.19476722e-01  1.32992511e-01 -8.48065099e-02\n  -2.56265713e-02 -1.91834580e-01  6.27339540e-03 -5.64965295e-02\n  -9.35266444e-04  1.19282180e-01  1.46881605e-01  1.30969552e-01\n  -5.58899464e-02 -6.13075105e-02  5.27830257e-02  2.16502787e-01\n   9.98940042e-02 -1.58632818e-02 -8.67458477e-02  1.93990322e-01\n   1.87216427e-01  1.57885167e-01 -7.86033138e-02  1.49318279e-01\n   1.39339544e-01  6.85627332e-02 -8.37773584e-02  1.56110348e-02\n  -8.59793202e-02 -1.65096035e-02  1.12676217e-01  3.13080540e-01\n   2.30281430e-01 -4.75722195e-02  1.53067535e-01 -1.31589458e-02\n   5.28508338e-02 -4.54004575e-02  2.01197578e-01  4.33739557e-02\n  -7.29970416e-02  9.84323213e-02  1.16889537e-02  1.57025818e-01\n  -6.02916611e-03 -2.16529112e-01  1.10212550e-01  1.25890916e-01\n  -5.32954816e-03  1.21465376e-01  1.08199136e-01  1.08866829e-01\n   1.45378193e-01  1.49490260e-01  3.41931414e-01 -7.49001438e-02\n  -2.44712426e-01  7.97465653e-02  9.89206677e-02  1.47479895e-01\n   2.61734645e-02 -4.48604278e-02  1.32294557e-01  2.85052221e-03]]\nLayer 3 weights:\n[[ 0.15208373  0.2893729   0.26844223 ... -0.76337702  0.33200565\n  -0.00689376]\n [-0.1135869  -0.25553247 -0.72970514 ...  0.05729071  0.04524448\n   0.73245364]\n [-0.25052317 -0.06862443  0.63856732 ... -0.23876534 -0.33084585\n  -0.03475201]\n ...\n [-0.02699442  0.27990826 -0.34338149 ...  0.22955553  0.1770323\n  -0.54687932]\n [ 0.4532182  -0.07568988  0.14761061 ... -0.24689195  0.59586536\n  -1.19850341]\n [ 0.06271953 -0.23864948  0.06340944 ... -0.02653471  0.22527828\n   0.33083839]]\nLayer 3 biases:\n[[-0.37010407  0.12386026 -0.22677005 -0.32644242  0.35068725  0.28752409\n  -0.16468803 -0.15973085  0.43038847  0.05527535]]\nEpoch 13/25, Loss: 0.011501410704180554\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.37375309e-01  7.68130142e-02  7.54792386e-02  4.09346608e-03\n  -2.14088901e-02  2.19007970e-02 -6.99026410e-02 -2.66043990e-03\n  -3.59517124e-02 -1.42813740e-01 -2.01071460e-01  5.73261891e-02\n   2.49291285e-01 -1.57452385e-02  2.52505541e-01 -1.57988907e-01\n  -8.28986747e-02 -1.02918862e-01  1.48902784e-02 -1.69702128e-02\n  -1.40332154e-01  6.63895177e-02  9.51882396e-05 -1.76861437e-01\n  -2.81832717e-02 -2.92289179e-02 -9.74143950e-02 -2.89852846e-01\n  -5.46984822e-02  2.37636900e-01 -2.13099250e-02  6.89828487e-02\n   1.09658282e-01  6.48547228e-02  1.17595031e-01  1.90108553e-02\n   2.34471428e-01  6.97084681e-02 -3.97156213e-02 -6.54067138e-02\n   2.12219449e-01  7.24334198e-02  7.45571823e-02 -5.23381648e-03\n  -1.17700691e-01 -4.90128609e-02  9.54932009e-02  2.31991891e-02\n  -2.05943569e-01 -4.03188029e-02 -5.65904034e-03 -1.07864712e-01\n  -9.96636335e-03  2.85592712e-02 -9.00994249e-02  1.49993268e-01\n  -8.03654296e-02  2.41016627e-01  3.97260387e-02 -2.56733653e-01\n  -1.61373036e-01  5.13309116e-03 -1.69506490e-01  3.49151189e-03\n   1.06643580e-01 -1.15926114e-01  1.33285704e-01 -8.29026001e-02\n  -2.35295507e-02 -1.86821992e-01  1.10767618e-02 -5.14615364e-02\n  -1.02986088e-03  1.18875126e-01  1.44750014e-01  1.33311550e-01\n  -5.94296047e-02 -5.46369177e-02  4.75009056e-02  2.22205494e-01\n   1.06320598e-01 -2.43812348e-02 -8.60430444e-02  1.94605952e-01\n   1.85550574e-01  1.67081540e-01 -8.24508318e-02  1.46961291e-01\n   1.46620684e-01  6.84153200e-02 -8.66959362e-02  1.54886678e-02\n  -9.28347710e-02 -1.75593553e-02  1.13461218e-01  3.17439150e-01\n   2.36618949e-01 -4.75306370e-02  1.55090512e-01 -2.65532883e-02\n   4.86865800e-02 -5.45949472e-02  2.07859071e-01  4.84520371e-02\n  -8.46329214e-02  1.00692480e-01  1.90654485e-02  1.57478432e-01\n  -2.76704878e-03 -2.00658891e-01  1.03463405e-01  1.23161970e-01\n  -3.12025694e-03  1.26194590e-01  1.04912771e-01  1.13181660e-01\n   1.35104003e-01  1.68511752e-01  3.44177021e-01 -7.28851777e-02\n  -2.45068344e-01  8.30030937e-02  1.03187354e-01  1.55937476e-01\n   2.52285531e-02 -4.19266564e-02  1.39171852e-01  2.88759847e-03]]\nLayer 3 weights:\n[[ 1.53461592e-01  3.01427900e-01  2.91794162e-01 ... -8.03328012e-01\n   3.16752876e-01  8.32491247e-04]\n [-1.15693614e-01 -2.57262737e-01 -7.42837591e-01 ...  4.63648111e-02\n   3.48543161e-02  7.20928092e-01]\n [-2.57914714e-01 -7.30948687e-02  6.49564912e-01 ... -2.43171889e-01\n  -3.34792828e-01 -3.45061657e-02]\n ...\n [-2.75658364e-02  3.03852148e-01 -3.56972706e-01 ...  2.36671124e-01\n   1.79549799e-01 -5.72791478e-01]\n [ 4.42034514e-01 -5.48422860e-02  1.37531395e-01 ... -2.54564539e-01\n   6.01197686e-01 -1.22911694e+00]\n [ 6.41277539e-02 -2.17861985e-01  6.56488957e-02 ... -6.21623554e-02\n   2.28185382e-01  3.49677984e-01]]\nLayer 3 biases:\n[[-0.37500846  0.12803745 -0.22286288 -0.32114343  0.35129388  0.2895907\n  -0.17010078 -0.16896714  0.43141621  0.05774445]]\nEpoch 14/25, Loss: 0.013664044868317767\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.36429903e-01  7.95435706e-02  7.71176413e-02  5.00135277e-03\n  -2.14919389e-02  1.82696140e-02 -7.20889288e-02 -2.40581631e-03\n  -4.17936282e-02 -1.47506621e-01 -1.99568866e-01  6.89825646e-02\n   2.52990661e-01 -2.03222842e-02  2.53859481e-01 -1.52851149e-01\n  -8.38906774e-02 -1.01926087e-01  8.18066868e-03 -1.12101882e-02\n  -1.41926003e-01  7.56000322e-02 -1.94033041e-04 -1.86891980e-01\n  -2.43249351e-02 -3.20328073e-02 -9.78032035e-02 -2.83392305e-01\n  -5.08886555e-02  2.43046320e-01 -2.25394650e-02  7.35748162e-02\n   1.19168953e-01  7.52126135e-02  1.08833124e-01  1.83823099e-02\n   2.36897542e-01  6.95996222e-02 -3.56207711e-02 -7.10991211e-02\n   2.12306437e-01  7.51342815e-02  7.39105421e-02 -5.22331045e-03\n  -1.19860155e-01 -4.83153543e-02  9.59417684e-02  2.02879051e-02\n  -2.09392575e-01 -4.19556293e-02 -1.17574934e-02 -1.18372149e-01\n  -3.21803254e-03  3.40535630e-02 -8.77589873e-02  1.40934374e-01\n  -8.33079039e-02  2.37817002e-01  3.68635055e-02 -2.57767141e-01\n  -1.57524207e-01  5.14244503e-03 -1.69907374e-01  2.13981491e-03\n   1.09474680e-01 -1.13021325e-01  1.21912256e-01 -8.12365947e-02\n  -2.58904376e-02 -1.80380627e-01  1.22029370e-02 -5.52327706e-02\n  -8.94393178e-04  1.24062625e-01  1.56290037e-01  1.33671667e-01\n  -4.72122658e-02 -5.23813217e-02  4.96583694e-02  2.22542204e-01\n   1.08865229e-01 -2.05337253e-02 -8.46423597e-02  2.00315023e-01\n   1.88496593e-01  1.68666662e-01 -8.52553970e-02  1.44853564e-01\n   1.39388495e-01  6.69260518e-02 -8.30073619e-02  8.03297466e-03\n  -8.66659533e-02 -1.67889466e-02  1.15693566e-01  3.10641571e-01\n   2.36961976e-01 -4.83721967e-02  1.59433669e-01 -3.24414921e-02\n   6.03395585e-02 -5.28537960e-02  2.20773747e-01  4.90034505e-02\n  -8.42639596e-02  1.01710806e-01  1.81933996e-02  1.63714129e-01\n  -6.55063800e-03 -2.04811025e-01  9.92818346e-02  1.19761097e-01\n  -7.05976123e-03  1.32488139e-01  1.06126908e-01  1.11105471e-01\n   1.45298883e-01  1.68580164e-01  3.57424851e-01 -8.38414799e-02\n  -2.39654331e-01  8.30969755e-02  1.03552737e-01  1.59029450e-01\n   2.50131909e-02 -3.87569458e-02  1.41014742e-01  1.38149802e-03]]\nLayer 3 weights:\n[[ 0.15972969  0.32305866  0.31250446 ... -0.82847302  0.30702512\n  -0.0019436 ]\n [-0.11569992 -0.25897717 -0.75513369 ...  0.04371353  0.05998275\n   0.72922441]\n [-0.26329531 -0.07359509  0.65900462 ... -0.24611007 -0.33906916\n  -0.03396459]\n ...\n [-0.02809418  0.31378403 -0.35804896 ...  0.24379374  0.18557368\n  -0.59535601]\n [ 0.44246923 -0.0747412   0.1407259  ... -0.24765763  0.63538938\n  -1.2438084 ]\n [ 0.06491401 -0.23497264  0.07467496 ... -0.0672564   0.24588368\n   0.35713901]]\nLayer 3 biases:\n[[-0.37715878  0.12697616 -0.21884999 -0.32858446  0.34873826  0.29027863\n  -0.17085606 -0.17402268  0.44218569  0.06129322]]\nEpoch 15/25, Loss: 0.00487180073110783\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.35233306e-01  8.62174631e-02  8.00401380e-02  5.04345643e-03\n  -1.89435715e-02  2.32878089e-02 -6.71615084e-02 -2.45045039e-03\n  -4.29409351e-02 -1.50210863e-01 -2.02485859e-01  6.64184002e-02\n   2.58579378e-01 -2.40481290e-02  2.51497170e-01 -1.40732492e-01\n  -7.83680494e-02 -9.99237298e-02  1.45765264e-02 -1.04133745e-02\n  -1.32651231e-01  8.51876620e-02 -1.33187359e-04 -1.84060618e-01\n  -2.97689533e-02 -3.13440009e-02 -9.83397885e-02 -2.80442206e-01\n  -4.85712659e-02  2.52223707e-01 -1.94597338e-02  7.04243433e-02\n   1.18910316e-01  6.90406155e-02  1.22091123e-01  1.99083344e-02\n   2.38601547e-01  7.29357353e-02 -3.60452218e-02 -7.11431055e-02\n   2.11281081e-01  7.83503823e-02  7.24984847e-02 -5.23254475e-03\n  -1.20646986e-01 -4.62553147e-02  1.00386171e-01  1.85613328e-02\n  -2.04430358e-01 -4.32240702e-02 -1.14486118e-02 -1.09132734e-01\n  -1.01377970e-02  3.70099402e-02 -8.78078287e-02  1.45419544e-01\n  -8.13242272e-02  2.43099286e-01  4.08122036e-02 -2.58291826e-01\n  -1.62609897e-01  5.92627795e-03 -1.69761448e-01  3.37389768e-03\n   1.02110573e-01 -1.16679278e-01  1.22806612e-01 -8.15374505e-02\n  -2.78165552e-02 -1.81088914e-01  1.69973137e-02 -5.14155971e-02\n  -8.31031836e-04  1.28968113e-01  1.52651655e-01  1.34512160e-01\n  -3.96098567e-02 -5.14409040e-02  4.96349903e-02  2.17530389e-01\n   1.03909428e-01 -2.96563650e-02 -8.68862779e-02  2.01912255e-01\n   1.95642273e-01  1.74208564e-01 -8.48466612e-02  1.44376362e-01\n   1.41837134e-01  7.30289771e-02 -8.11269178e-02  5.82686424e-03\n  -9.50132732e-02 -2.63927632e-02  1.25553724e-01  3.16745784e-01\n   2.42928270e-01 -5.14492196e-02  1.62987162e-01 -3.99166958e-02\n   5.95595494e-02 -5.17452156e-02  2.26773945e-01  5.02297323e-02\n  -8.40403910e-02  1.02945549e-01  1.94084571e-02  1.73125076e-01\n  -3.27831880e-03 -2.07086738e-01  9.79777011e-02  1.26269889e-01\n  -4.96733459e-03  1.37243466e-01  1.06963215e-01  1.11457825e-01\n   1.41821860e-01  1.71192549e-01  3.53768008e-01 -9.11222331e-02\n  -2.53014881e-01  8.73145280e-02  1.09296417e-01  1.67543777e-01\n   2.40923409e-02 -4.35786636e-02  1.42768973e-01  4.47217066e-03]]\nLayer 3 weights:\n[[ 0.15606969  0.32252639  0.32110274 ... -0.83815682  0.31460528\n   0.00667873]\n [-0.10497778 -0.26107611 -0.77672614 ...  0.04448246  0.05792261\n   0.74498765]\n [-0.26630846 -0.07572656  0.67054276 ... -0.2509562  -0.3428513\n  -0.03380097]\n ...\n [-0.03126969  0.32340473 -0.36810006 ...  0.24544527  0.19375714\n  -0.61429812]\n [ 0.4441962  -0.07439373  0.16093404 ... -0.24913905  0.64255487\n  -1.26234019]\n [ 0.0694477  -0.23927897  0.07393784 ... -0.07509851  0.24384624\n   0.3598182 ]]\nLayer 3 biases:\n[[-0.37573349  0.12418099 -0.218112   -0.33335282  0.35175928  0.29251188\n  -0.17340231 -0.17803874  0.44225957  0.06792763]]\nEpoch 16/25, Loss: 0.002653488353319305\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.34972995e-01  8.57702994e-02  7.76026157e-02  4.70328531e-03\n  -1.96301096e-02  1.70305814e-02 -7.63334330e-02 -2.43724837e-03\n  -4.41753720e-02 -1.48332095e-01 -2.03555124e-01  6.70823761e-02\n   2.61980907e-01 -2.51033052e-02  2.62743177e-01 -1.41734890e-01\n  -8.33579292e-02 -9.55322167e-02  8.00633720e-03 -5.56959018e-03\n  -1.36400399e-01  8.64381751e-02  2.28062522e-04 -1.88907766e-01\n  -2.94844349e-02 -3.07221229e-02 -9.72417112e-02 -2.76958962e-01\n  -4.43216901e-02  2.53821892e-01 -1.96578257e-02  7.32026781e-02\n   1.22728465e-01  6.53992945e-02  1.18312826e-01  2.07188161e-02\n   2.35577121e-01  7.47850009e-02 -3.50715684e-02 -7.21927127e-02\n   2.16099307e-01  8.16427256e-02  7.06127471e-02 -5.24419663e-03\n  -1.22434486e-01 -4.70827107e-02  9.88080005e-02  1.60410085e-02\n  -2.08389485e-01 -3.66369080e-02 -1.36004371e-02 -1.16053359e-01\n  -5.84484026e-03  3.14629599e-02 -8.30431472e-02  1.45864365e-01\n  -8.63376775e-02  2.48625155e-01  4.02390776e-02 -2.64368630e-01\n  -1.61976449e-01  6.01765007e-03 -1.70948192e-01  1.85745469e-03\n   1.11187707e-01 -1.17932750e-01  1.30969298e-01 -7.98204081e-02\n  -2.79654905e-02 -1.78303329e-01  1.64727219e-02 -5.33846621e-02\n  -7.05982268e-04  1.30013833e-01  1.49853155e-01  1.34471437e-01\n  -4.19418424e-02 -4.76067066e-02  5.02624964e-02  2.21100101e-01\n   1.04775610e-01 -2.46865248e-02 -8.17422295e-02  2.02772377e-01\n   1.94296837e-01  1.67869912e-01 -8.64166867e-02  1.37888261e-01\n   1.42361263e-01  8.14763875e-02 -7.78805802e-02  5.67807524e-03\n  -9.09968353e-02 -2.31426658e-02  1.17015334e-01  3.12485762e-01\n   2.44083094e-01 -4.80024509e-02  1.65975753e-01 -3.50290379e-02\n   5.48816071e-02 -4.89171261e-02  2.31419767e-01  5.43891880e-02\n  -8.07269179e-02  1.03822534e-01  2.07586538e-02  1.74148554e-01\n  -4.38172095e-03 -2.07532899e-01  1.03464182e-01  1.31345224e-01\n  -1.17582422e-02  1.37436842e-01  1.08923596e-01  1.13102417e-01\n   1.42152922e-01  1.68332236e-01  3.63703691e-01 -9.32950137e-02\n  -2.48079220e-01  8.87916809e-02  1.13797600e-01  1.67579381e-01\n   2.30994216e-02 -4.50756330e-02  1.51369214e-01  4.32621442e-03]]\nLayer 3 weights:\n[[ 0.15669872  0.33726823  0.32668869 ... -0.85551223  0.32272101\n   0.00257819]\n [-0.09870046 -0.2621011  -0.7860453  ...  0.04743636  0.06893996\n   0.74950731]\n [-0.26790518 -0.0757594   0.67478571 ... -0.25412594 -0.34422647\n  -0.03363901]\n ...\n [-0.02979537  0.33976518 -0.37827478 ...  0.24664956  0.20405568\n  -0.63019391]\n [ 0.46372735 -0.05831731  0.15604276 ... -0.25298327  0.65481422\n  -1.28442093]\n [ 0.0728593  -0.23989623  0.07115535 ... -0.07402085  0.24583489\n   0.37167238]]\nLayer 3 biases:\n[[-0.37291955  0.12851261 -0.22166414 -0.34006297  0.34890462  0.29263001\n  -0.17394184 -0.18046949  0.44675291  0.07225784]]\nEpoch 17/25, Loss: 0.004305511846852729\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.33902690e-01  8.69832145e-02  8.24937062e-02  5.26281019e-03\n  -2.32475471e-02  2.16534007e-02 -7.21647532e-02 -2.41364427e-03\n  -4.61584604e-02 -1.47601671e-01 -2.03620617e-01  6.50909591e-02\n   2.68084057e-01 -2.04332946e-02  2.64254991e-01 -1.44718753e-01\n  -8.31431702e-02 -9.69766660e-02  8.00880929e-03 -5.12622750e-03\n  -1.35781009e-01  8.48660626e-02  1.45256124e-04 -1.90957273e-01\n  -3.10028548e-02 -2.96040838e-02 -9.59530477e-02 -2.74705300e-01\n  -4.26869199e-02  2.59045090e-01 -1.38237703e-02  7.09929502e-02\n   1.30407691e-01  6.95803782e-02  1.22472637e-01  2.07407653e-02\n   2.36730803e-01  7.62823904e-02 -3.10100467e-02 -7.28345307e-02\n   2.10472622e-01  8.56772207e-02  7.42724627e-02 -5.24388515e-03\n  -1.23952905e-01 -4.59618948e-02  1.01532260e-01  2.01973338e-02\n  -2.07601722e-01 -4.20543807e-02 -1.05501453e-02 -1.11098725e-01\n  -3.60360802e-03  3.22509983e-02 -8.28057290e-02  1.45936281e-01\n  -9.00697746e-02  2.48092010e-01  4.26816856e-02 -2.67098900e-01\n  -1.62517171e-01  6.37522067e-03 -1.68407421e-01  2.66133104e-03\n   1.13403849e-01 -1.17106358e-01  1.27381082e-01 -7.72844501e-02\n  -2.88975531e-02 -1.77474095e-01  1.81217406e-02 -5.40158352e-02\n  -6.02422074e-04  1.32144154e-01  1.52452439e-01  1.35524612e-01\n  -4.30802061e-02 -4.59280882e-02  4.98459950e-02  2.21634498e-01\n   1.02040535e-01 -2.49643536e-02 -8.22398050e-02  2.04688377e-01\n   1.96781835e-01  1.72381051e-01 -9.19918815e-02  1.41755774e-01\n   1.42389499e-01  7.84322378e-02 -7.63583660e-02  5.52226407e-03\n  -9.41149099e-02 -2.54085669e-02  1.20250684e-01  3.09475471e-01\n   2.44110502e-01 -4.76142903e-02  1.69643465e-01 -3.11959567e-02\n   5.73710912e-02 -4.93625966e-02  2.38698011e-01  6.10384743e-02\n  -8.14971716e-02  1.05140732e-01  2.49570669e-02  1.78364911e-01\n   1.46931419e-03 -2.07084182e-01  1.07803713e-01  1.32256654e-01\n  -1.36569312e-02  1.41762104e-01  1.11048950e-01  1.15322727e-01\n   1.44498063e-01  1.70434481e-01  3.66401530e-01 -9.16797609e-02\n  -2.50452177e-01  9.03364770e-02  1.15646276e-01  1.63811660e-01\n   1.91254955e-02 -5.14310220e-02  1.46625732e-01  6.54535474e-03]]\nLayer 3 weights:\n[[ 0.16114368  0.33989675  0.33783237 ... -0.87522167  0.32784065\n   0.00305294]\n [-0.1075993  -0.26301879 -0.79330179 ...  0.04175446  0.06212367\n   0.76605841]\n [-0.27164985 -0.07785466  0.68315209 ... -0.25708304 -0.34797076\n  -0.03355838]\n ...\n [-0.03057182  0.34672969 -0.3848184  ...  0.25148861  0.20021321\n  -0.64350066]\n [ 0.46907675 -0.06476171  0.15522982 ... -0.25384654  0.6544638\n  -1.29725511]\n [ 0.07433274 -0.25513678  0.07949089 ... -0.07514026  0.23681482\n   0.38355257]]\nLayer 3 biases:\n[[-0.37488019  0.12660378 -0.21850305 -0.33936948  0.35310824  0.2958599\n  -0.17674058 -0.18499199  0.44526603  0.07364733]]\nEpoch 18/25, Loss: 0.0014052776089672095\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.33503957e-01  8.95691713e-02  8.33465641e-02  5.49350722e-03\n  -2.20892510e-02  1.80350129e-02 -7.28595009e-02 -2.22531441e-03\n  -4.75643777e-02 -1.49887397e-01 -2.01528448e-01  6.90233619e-02\n   2.70624098e-01 -2.14792746e-02  2.61583546e-01 -1.43926982e-01\n  -8.00987363e-02 -9.49805081e-02  6.28432343e-03 -3.66182038e-03\n  -1.34779925e-01  8.77824873e-02  6.22025025e-05 -1.94188576e-01\n  -3.04122050e-02 -2.91122226e-02 -9.56965794e-02 -2.77037494e-01\n  -4.12925802e-02  2.61690319e-01 -1.32803211e-02  7.18827019e-02\n   1.29879822e-01  6.86120811e-02  1.25665906e-01  2.04009295e-02\n   2.37399094e-01  7.78971806e-02 -3.48262525e-02 -7.32170955e-02\n   2.12055628e-01  8.89049488e-02  7.61128528e-02 -5.25798629e-03\n  -1.26011361e-01 -4.88951679e-02  1.01810387e-01  1.97283000e-02\n  -2.06261830e-01 -4.73375936e-02 -8.50866867e-03 -1.10820360e-01\n  -1.66209118e-03  3.35288651e-02 -8.28936257e-02  1.48212167e-01\n  -9.25352832e-02  2.52191994e-01  4.01562967e-02 -2.67857127e-01\n  -1.61427081e-01  6.44716593e-03 -1.72952779e-01  3.57707054e-03\n   1.17280224e-01 -1.18321780e-01  1.27164540e-01 -7.61343880e-02\n  -2.93391271e-02 -1.78207252e-01  2.05234057e-02 -5.43638955e-02\n  -5.57449608e-04  1.34631414e-01  1.57007387e-01  1.34699228e-01\n  -4.13902912e-02 -4.64712607e-02  5.44377885e-02  2.24578605e-01\n   1.05304105e-01 -2.84655712e-02 -8.30708579e-02  2.08607398e-01\n   1.98131642e-01  1.70052008e-01 -9.63570376e-02  1.42522012e-01\n   1.42587988e-01  7.94520500e-02 -7.55388574e-02  5.22705085e-03\n  -9.53613050e-02 -2.57242594e-02  1.16701309e-01  3.12871757e-01\n   2.50192943e-01 -4.92100149e-02  1.71397523e-01 -3.72948077e-02\n   5.94598687e-02 -5.20759207e-02  2.42143828e-01  6.02927368e-02\n  -8.53118173e-02  1.05203390e-01  2.14779110e-02  1.78441587e-01\n   1.51228288e-04 -2.01549080e-01  1.05642178e-01  1.31010914e-01\n  -1.27182278e-02  1.47755295e-01  1.09512061e-01  1.16819280e-01\n   1.47016560e-01  1.72981724e-01  3.69832504e-01 -9.03056949e-02\n  -2.50698823e-01  9.23885479e-02  1.16708145e-01  1.72239175e-01\n   2.19030018e-02 -4.88690164e-02  1.49704510e-01  5.33414495e-03]]\nLayer 3 weights:\n[[ 0.1649616   0.33824369  0.34005439 ... -0.88589759  0.32963792\n   0.00216324]\n [-0.10558613 -0.26437214 -0.80361757 ...  0.0486146   0.05705614\n   0.77498376]\n [-0.27381023 -0.07920997  0.69019538 ... -0.25944505 -0.35085466\n  -0.03317893]\n ...\n [-0.02782676  0.35221253 -0.39341367 ...  0.25564834  0.2045772\n  -0.65572275]\n [ 0.48160371 -0.06929323  0.15532285 ... -0.25260721  0.66573911\n  -1.31361715]\n [ 0.07765659 -0.2596307   0.08374966 ... -0.08253242  0.24007328\n   0.39169618]]\nLayer 3 biases:\n[[-0.37281284  0.12227603 -0.2169264  -0.34053268  0.35411279  0.29946981\n  -0.17974595 -0.18554748  0.44742332  0.07228341]]\nEpoch 19/25, Loss: 0.0014816231449170814\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.38196222e-01  8.95303773e-02  8.46551499e-02  5.56323559e-03\n  -2.32435840e-02  1.79061056e-02 -6.96621476e-02 -2.17827352e-03\n  -5.07158918e-02 -1.45015484e-01 -2.04027321e-01  7.03105642e-02\n   2.73533008e-01 -2.35728086e-02  2.64553273e-01 -1.43741442e-01\n  -8.08826241e-02 -9.68857268e-02  4.24555364e-03 -3.86234015e-03\n  -1.34578931e-01  8.72106912e-02  3.50488845e-05 -1.92651506e-01\n  -3.50168559e-02 -2.92646455e-02 -9.63522338e-02 -2.70225001e-01\n  -4.32381681e-02  2.60067419e-01 -1.20808370e-02  7.39295299e-02\n   1.31190336e-01  6.81001232e-02  1.25279696e-01  2.09784917e-02\n   2.37880027e-01  7.89730972e-02 -3.49322458e-02 -7.03979543e-02\n   2.13322915e-01  8.85838542e-02  7.50253459e-02 -5.25071646e-03\n  -1.29342725e-01 -4.71181200e-02  1.00043511e-01  1.81949416e-02\n  -2.06241403e-01 -4.38643116e-02 -1.19432234e-02 -1.13734054e-01\n  -1.55563531e-03  3.57963030e-02 -8.02438242e-02  1.49213294e-01\n  -9.02661638e-02  2.52664146e-01  4.16560690e-02 -2.74338662e-01\n  -1.63652320e-01  6.61572057e-03 -1.71463732e-01  3.98692666e-03\n   1.18919130e-01 -1.22576600e-01  1.30064661e-01 -7.55595411e-02\n  -2.84222805e-02 -1.74236530e-01  2.34414003e-02 -5.47617758e-02\n  -5.45387951e-04  1.35614950e-01  1.54254600e-01  1.35864148e-01\n  -4.07858953e-02 -4.68535243e-02  5.35177303e-02  2.24014274e-01\n   1.07161457e-01 -2.88915720e-02 -8.19930749e-02  2.07625885e-01\n   1.94527451e-01  1.72011074e-01 -9.46786111e-02  1.38932484e-01\n   1.44846995e-01  8.34131606e-02 -7.41538646e-02  4.90910390e-03\n  -9.52288074e-02 -2.46318376e-02  1.18079172e-01  3.14076680e-01\n   2.49514559e-01 -4.76752754e-02  1.74026008e-01 -4.01894573e-02\n   5.68902786e-02 -4.99164741e-02  2.42766715e-01  6.54308425e-02\n  -8.57191697e-02  1.06375293e-01  2.25215416e-02  1.81062780e-01\n  -2.01429503e-03 -2.03476348e-01  1.07854240e-01  1.34812260e-01\n  -1.36097816e-02  1.48676476e-01  1.11123062e-01  1.17881984e-01\n   1.45887852e-01  1.72576791e-01  3.75598131e-01 -8.85792041e-02\n  -2.50083284e-01  9.41366035e-02  1.20313952e-01  1.72691143e-01\n   2.05864127e-02 -4.66953618e-02  1.49796165e-01  5.38471988e-03]]\nLayer 3 weights:\n[[ 1.66188384e-01  3.45786949e-01  3.42875020e-01 ... -8.93316697e-01\n   3.28994057e-01  1.92522594e-04]\n [-1.09191375e-01 -2.65005869e-01 -8.12823229e-01 ...  4.91434978e-02\n   5.82403465e-02  7.95443688e-01]\n [-2.76801699e-01 -7.89681331e-02  6.95279191e-01 ... -2.61680855e-01\n  -3.53525973e-01 -3.26453325e-02]\n ...\n [-2.70971302e-02  3.60513464e-01 -3.99856661e-01 ...  2.58614848e-01\n   2.09810284e-01 -6.67326669e-01]\n [ 4.87513007e-01 -6.78635828e-02  1.57978669e-01 ... -2.56066283e-01\n   6.77552207e-01 -1.32523257e+00]\n [ 7.79821496e-02 -2.58549674e-01  8.04851741e-02 ... -8.16502717e-02\n   2.44650441e-01  3.96705065e-01]]\nLayer 3 biases:\n[[-0.37290686  0.12312501 -0.21908915 -0.34409511  0.35253668  0.29914443\n  -0.17873521 -0.1847522   0.44871035  0.07606205]]\nEpoch 20/25, Loss: 0.0011875976003857813\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.34448490e-01  9.34645578e-02  8.61069073e-02  5.71832508e-03\n  -2.02618059e-02  1.96432457e-02 -6.93596092e-02 -2.20169521e-03\n  -4.90190122e-02 -1.49057206e-01 -2.03254453e-01  6.80037581e-02\n   2.76727441e-01 -1.97614157e-02  2.63444043e-01 -1.42751499e-01\n  -8.12347423e-02 -9.44502352e-02  3.93703697e-03 -2.24573436e-03\n  -1.34859909e-01  8.79176298e-02  1.68992272e-04 -1.96072420e-01\n  -3.29248627e-02 -2.90250660e-02 -9.65738261e-02 -2.71708989e-01\n  -4.17556142e-02  2.63989287e-01 -1.12729728e-02  7.30041962e-02\n   1.36310022e-01  6.97800689e-02  1.21700978e-01  2.10864460e-02\n   2.38837443e-01  7.95335182e-02 -3.47182971e-02 -7.14279521e-02\n   2.13897193e-01  9.04694819e-02  7.32413558e-02 -5.24016972e-03\n  -1.28733237e-01 -4.74559996e-02  1.00585320e-01  1.93398163e-02\n  -2.07047619e-01 -4.51914351e-02 -9.47696435e-03 -1.13283862e-01\n   2.72065538e-04  3.66724484e-02 -8.28404789e-02  1.51692385e-01\n  -9.05462169e-02  2.54429115e-01  4.35320638e-02 -2.73673504e-01\n  -1.63131184e-01  6.87981221e-03 -1.70493449e-01  3.65650004e-03\n   1.20736243e-01 -1.17296059e-01  1.27988514e-01 -7.51951781e-02\n  -3.01339161e-02 -1.72842502e-01  2.47554519e-02 -5.42197701e-02\n  -5.20456722e-04  1.34894169e-01  1.56356309e-01  1.36616827e-01\n  -4.43841745e-02 -4.10261690e-02  5.27642724e-02  2.24587141e-01\n   1.07133453e-01 -2.89046155e-02 -8.05973376e-02  2.12212091e-01\n   1.94852643e-01  1.71894951e-01 -9.74908188e-02  1.39095902e-01\n   1.47386206e-01  8.40774327e-02 -7.41852411e-02  3.41834483e-03\n  -9.53864259e-02 -2.53306566e-02  1.19568661e-01  3.15791629e-01\n   2.55624807e-01 -5.24072535e-02  1.76569160e-01 -3.77917318e-02\n   5.69379331e-02 -5.00659470e-02  2.49176271e-01  6.41104742e-02\n  -8.23245286e-02  1.07959383e-01  2.28176401e-02  1.82673949e-01\n   1.99299147e-03 -2.01957574e-01  1.09428198e-01  1.34002766e-01\n  -1.31922113e-02  1.51066733e-01  1.10666707e-01  1.17516212e-01\n   1.47963099e-01  1.78799345e-01  3.73252870e-01 -8.99182259e-02\n  -2.49755222e-01  9.45431809e-02  1.24606325e-01  1.74715382e-01\n   1.90395589e-02 -5.27486133e-02  1.50543827e-01  6.50625217e-03]]\nLayer 3 weights:\n[[ 0.16592149  0.35047543  0.34809059 ... -0.90261584  0.33019706\n   0.00337763]\n [-0.11173807 -0.26550357 -0.8197146  ...  0.05530593  0.05575673\n   0.79526018]\n [-0.27932908 -0.07976914  0.70116882 ... -0.26402017 -0.35581354\n  -0.03234087]\n ...\n [-0.02814675  0.36480027 -0.40270232 ...  0.2582208   0.21429196\n  -0.67420356]\n [ 0.48839941 -0.06900236  0.16174411 ... -0.25949008  0.68323856\n  -1.33459922]\n [ 0.07744412 -0.2654082   0.0849478  ... -0.0845413   0.24526344\n   0.39985262]]\nLayer 3 biases:\n[[-0.37460752  0.12462925 -0.21724108 -0.34452011  0.35277484  0.29875875\n  -0.1785701  -0.18768208  0.44962347  0.07683457]]\nEpoch 21/25, Loss: 0.0009559408802894135\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.33384083e-01  9.41679390e-02  8.57534847e-02  5.73597357e-03\n  -2.12116964e-02  1.68234025e-02 -6.92452531e-02 -2.19761011e-03\n  -5.07950436e-02 -1.47717843e-01 -2.01515262e-01  6.61605581e-02\n   2.79583079e-01 -2.22774436e-02  2.64412557e-01 -1.42099680e-01\n  -8.36961849e-02 -9.47629603e-02  5.83295928e-03 -1.89904048e-03\n  -1.32552788e-01  8.94921539e-02  1.73697549e-05 -1.97279759e-01\n  -3.59949260e-02 -2.72920302e-02 -9.53352449e-02 -2.70565949e-01\n  -3.95287683e-02  2.65417791e-01 -9.16812563e-03  7.28375353e-02\n   1.37365340e-01  6.86365267e-02  1.24156655e-01  2.11049239e-02\n   2.40975628e-01  8.18272451e-02 -3.39616906e-02 -7.03116262e-02\n   2.15052958e-01  9.10505906e-02  7.38500741e-02 -5.24570045e-03\n  -1.29483968e-01 -4.74813186e-02  1.01282739e-01  1.85047899e-02\n  -2.07003829e-01 -4.71385342e-02 -1.04256137e-02 -1.14296171e-01\n   1.39545021e-05  3.54564843e-02 -8.23760737e-02  1.50120539e-01\n  -9.24396743e-02  2.56569754e-01  4.39074699e-02 -2.75309624e-01\n  -1.61761880e-01  6.98437124e-03 -1.72220867e-01  3.96580680e-03\n   1.22234882e-01 -1.18713327e-01  1.30960857e-01 -7.41258521e-02\n  -3.01435149e-02 -1.71368106e-01  2.49000407e-02 -5.38780462e-02\n  -5.14651880e-04  1.35474799e-01  1.55720560e-01  1.37018296e-01\n  -4.02210580e-02 -4.26164692e-02  5.28264014e-02  2.26245956e-01\n   1.05364906e-01 -2.87803132e-02 -8.11188711e-02  2.11606261e-01\n   1.97451300e-01  1.70864743e-01 -9.69402877e-02  1.37645061e-01\n   1.47662185e-01  8.52797296e-02 -7.29451776e-02  4.41065671e-03\n  -9.81249194e-02 -2.61775549e-02  1.20202633e-01  3.15186988e-01\n   2.54619841e-01 -5.29304801e-02  1.77630784e-01 -4.24820946e-02\n   5.84062120e-02 -4.77956934e-02  2.54099195e-01  6.58617031e-02\n  -8.45056690e-02  1.08761292e-01  2.32200017e-02  1.84916170e-01\n   2.49063256e-04 -1.99020019e-01  1.10606606e-01  1.36717421e-01\n  -1.38980482e-02  1.52164632e-01  1.15215071e-01  1.20496539e-01\n   1.51108396e-01  1.75174205e-01  3.77367141e-01 -9.21398728e-02\n  -2.52806378e-01  9.73013160e-02  1.26180047e-01  1.73029093e-01\n   1.73196592e-02 -5.12440212e-02  1.51953332e-01  6.35411048e-03]]\nLayer 3 weights:\n[[ 0.16847197  0.35434422  0.35451311 ... -0.91727402  0.33469252\n   0.00299805]\n [-0.10897627 -0.26613263 -0.8267255  ...  0.05031336  0.05656403\n   0.80595379]\n [-0.28141238 -0.08026489  0.70463328 ... -0.26547962 -0.35753539\n  -0.03196521]\n ...\n [-0.02835538  0.37218221 -0.40820057 ...  0.26225108  0.21536954\n  -0.6847966 ]\n [ 0.49304202 -0.06764087  0.15745963 ... -0.25646491  0.69144734\n  -1.34633761]\n [ 0.08026906 -0.26727653  0.08098059 ... -0.08402874  0.24625838\n   0.40247557]]\nLayer 3 biases:\n[[-0.37375475  0.12402379 -0.21634803 -0.34674579  0.35322864  0.30026998\n  -0.18017898 -0.19004471  0.45190583  0.07764403]]\nEpoch 22/25, Loss: 0.0013809006818809293\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.32643963e-01  9.56094345e-02  8.78231048e-02  6.07757827e-03\n  -1.96033787e-02  2.03518171e-02 -7.09766106e-02 -2.16342869e-03\n  -5.19121842e-02 -1.48717965e-01 -2.01440157e-01  6.58092009e-02\n   2.80017519e-01 -2.09566081e-02  2.68455680e-01 -1.40194822e-01\n  -8.34609436e-02 -9.26673754e-02  6.51266852e-03 -1.41468064e-03\n  -1.35109160e-01  9.08827328e-02  7.97063293e-05 -1.98745965e-01\n  -3.51738662e-02 -2.63756537e-02 -9.39076343e-02 -2.69582175e-01\n  -3.85781940e-02  2.66322452e-01 -8.92001880e-03  7.34005558e-02\n   1.37473842e-01  6.91869345e-02  1.26659821e-01  2.15127813e-02\n   2.41488908e-01  8.28161534e-02 -3.45752856e-02 -7.02379203e-02\n   2.12827356e-01  9.30459392e-02  7.32050635e-02 -5.23656916e-03\n  -1.32146323e-01 -4.86914820e-02  1.00242833e-01  1.95673718e-02\n  -2.06968944e-01 -4.63607767e-02 -9.46132899e-03 -1.13189337e-01\n  -2.22699036e-03  3.81626658e-02 -8.16531241e-02  1.51885837e-01\n  -9.25874123e-02  2.57980981e-01  4.27857163e-02 -2.75282930e-01\n  -1.64535919e-01  7.05434059e-03 -1.72831335e-01  4.70457956e-03\n   1.20542975e-01 -1.18668277e-01  1.28061684e-01 -7.37559331e-02\n  -2.95782724e-02 -1.71083084e-01  2.56291006e-02 -5.39837351e-02\n  -4.89575687e-04  1.38232235e-01  1.57351127e-01  1.37707890e-01\n  -4.13016024e-02 -4.19022598e-02  5.34994311e-02  2.26254534e-01\n   1.05235184e-01 -2.98348798e-02 -8.35492306e-02  2.13384686e-01\n   1.97601731e-01  1.73475069e-01 -1.00826059e-01  1.38003346e-01\n   1.47358955e-01  8.49281851e-02 -7.27906351e-02  5.47713612e-03\n  -9.74453084e-02 -2.68704841e-02  1.18786892e-01  3.12940984e-01\n   2.58627244e-01 -5.16132665e-02  1.78205380e-01 -4.18266632e-02\n   5.71410441e-02 -4.76040644e-02  2.57398603e-01  6.86457409e-02\n  -8.54042025e-02  1.08330024e-01  2.44129519e-02  1.86104618e-01\n   3.19988931e-03 -1.97867347e-01  1.11472294e-01  1.36601763e-01\n  -1.18853942e-02  1.52270834e-01  1.14125002e-01  1.19301060e-01\n   1.48199789e-01  1.78484539e-01  3.76627495e-01 -9.41035131e-02\n  -2.54077361e-01  9.78218705e-02  1.25341563e-01  1.76112171e-01\n   1.88897388e-02 -5.30627431e-02  1.54903289e-01  5.94275165e-03]]\nLayer 3 weights:\n[[ 1.69465057e-01  3.59391204e-01  3.56082614e-01 ... -9.26118236e-01\n   3.36087280e-01  9.23871324e-04]\n [-1.12537858e-01 -2.66836705e-01 -8.31277814e-01 ...  5.19763380e-02\n   5.61474221e-02  8.11097687e-01]\n [-2.83562296e-01 -8.13314088e-02  7.09664250e-01 ... -2.67397925e-01\n  -3.59972298e-01 -3.16687665e-02]\n ...\n [-2.63462665e-02  3.75818353e-01 -4.10996122e-01 ...  2.63825929e-01\n   2.17720613e-01 -6.93847166e-01]\n [ 5.01311328e-01 -7.16935101e-02  1.60792800e-01 ... -2.58892715e-01\n   7.02115640e-01 -1.35695095e+00]\n [ 8.03877739e-02 -2.71404634e-01  8.62843695e-02 ... -9.08844927e-02\n   2.47434280e-01  4.04101165e-01]]\nLayer 3 biases:\n[[-0.37381112  0.12384976 -0.21457142 -0.34706815  0.3553685   0.30041123\n  -0.18159469 -0.19235521  0.45295694  0.07681415]]\nEpoch 23/25, Loss: 0.0015250293505149903\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.34099283e-01  9.77388577e-02  8.87407868e-02  5.99987734e-03\n  -2.04205002e-02  1.80944940e-02 -6.81435093e-02 -2.21738660e-03\n  -5.24885786e-02 -1.49365857e-01 -2.02528347e-01  6.48040997e-02\n   2.82523664e-01 -2.17535950e-02  2.71174699e-01 -1.39944455e-01\n  -8.25650949e-02 -9.30251739e-02  5.57549392e-03 -1.10956725e-03\n  -1.32234507e-01  9.27931786e-02  5.10801269e-05 -1.97989369e-01\n  -3.69545834e-02 -2.52836624e-02 -9.31305445e-02 -2.69915328e-01\n  -4.00551657e-02  2.68723819e-01 -6.48341602e-03  7.08505308e-02\n   1.40062315e-01  7.02661382e-02  1.27957342e-01  2.15434117e-02\n   2.42130571e-01  8.35701721e-02 -3.25173681e-02 -7.04304887e-02\n   2.14499594e-01  9.31225088e-02  7.33060708e-02 -5.24714293e-03\n  -1.31378060e-01 -4.75625599e-02  1.01022424e-01  1.77630450e-02\n  -2.06073677e-01 -4.85945910e-02 -1.02808034e-02 -1.14227325e-01\n  -1.53417290e-04  3.81919273e-02 -8.14090319e-02  1.51947416e-01\n  -9.28306278e-02  2.57382356e-01  4.41597425e-02 -2.75582801e-01\n  -1.64661976e-01  7.21314135e-03 -1.71826886e-01  4.28832098e-03\n   1.23192682e-01 -1.15667012e-01  1.29957725e-01 -7.33412263e-02\n  -3.09603056e-02 -1.70099205e-01  2.67184741e-02 -5.40517613e-02\n  -4.47174365e-04  1.38469831e-01  1.55865424e-01  1.37242009e-01\n  -4.19346685e-02 -3.87857254e-02  5.38482585e-02  2.27744291e-01\n   1.05302893e-01 -2.87063171e-02 -8.46117594e-02  2.14130337e-01\n   1.96436321e-01  1.71613342e-01 -9.98492224e-02  1.38746536e-01\n   1.47898402e-01  8.76752100e-02 -7.13750275e-02  4.42154722e-03\n  -9.96633087e-02 -2.61504501e-02  1.21670549e-01  3.16635184e-01\n   2.59231066e-01 -4.99357120e-02  1.78854451e-01 -4.59065394e-02\n   5.75718483e-02 -4.77420854e-02  2.58743120e-01  6.86266580e-02\n  -8.48403357e-02  1.08726145e-01  2.39791755e-02  1.89744089e-01\n   1.04210616e-03 -1.97493717e-01  1.11243722e-01  1.37396335e-01\n  -1.33592154e-02  1.53678244e-01  1.15245362e-01  1.20507806e-01\n   1.52190911e-01  1.78689202e-01  3.80603585e-01 -9.58344893e-02\n  -2.56521340e-01  9.87378256e-02  1.26664720e-01  1.76037482e-01\n   1.67941060e-02 -5.20963210e-02  1.51378668e-01  8.04907847e-03]]\nLayer 3 weights:\n[[ 0.17094095  0.3621259   0.35963984 ... -0.9343363   0.33712101\n   0.0034362 ]\n [-0.11282221 -0.26713677 -0.83477126 ...  0.04963874  0.05569307\n   0.8207091 ]\n [-0.28585326 -0.08208905  0.71380623 ... -0.26807865 -0.36178938\n  -0.03136589]\n ...\n [-0.02657454  0.38078807 -0.41590122 ...  0.26617216  0.21913997\n  -0.70314248]\n [ 0.50380405 -0.07062562  0.15967287 ... -0.25699274  0.70062383\n  -1.36576343]\n [ 0.08122798 -0.27587435  0.08766133 ... -0.0882243   0.24654726\n   0.40922483]]\nLayer 3 biases:\n[[-0.37367421  0.12328794 -0.21417114 -0.35011551  0.35531937  0.30042876\n  -0.18286577 -0.19225647  0.45349194  0.0805551 ]]\nEpoch 24/25, Loss: 0.0010486909522863452\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.34372528e-01  9.75532374e-02  8.95678046e-02  6.16167027e-03\n  -1.99054685e-02  1.66155297e-02 -6.82888834e-02 -2.18764422e-03\n  -5.36134876e-02 -1.49394412e-01 -2.02138965e-01  6.56074637e-02\n   2.84616876e-01 -2.28365992e-02  2.71078888e-01 -1.39051882e-01\n  -8.55065345e-02 -9.20254978e-02  6.17913593e-03 -1.24708959e-03\n  -1.33041586e-01  9.06461222e-02  5.91618470e-05 -1.98874643e-01\n  -3.73655754e-02 -2.63539584e-02 -9.33967210e-02 -2.66935075e-01\n  -3.95291294e-02  2.70676657e-01 -6.36039755e-03  7.11189834e-02\n   1.40089799e-01  7.10509143e-02  1.28359233e-01  2.17430941e-02\n   2.43889421e-01  8.39434916e-02 -3.42667893e-02 -7.06682727e-02\n   2.15334476e-01  9.44153572e-02  7.35941016e-02 -5.25251660e-03\n  -1.32990908e-01 -4.84351180e-02  1.01467553e-01  1.77675712e-02\n  -2.05389379e-01 -4.87969973e-02 -1.10795912e-02 -1.15423062e-01\n  -2.09111375e-04  3.84402823e-02 -8.14338683e-02  1.53593656e-01\n  -9.31544577e-02  2.59760901e-01  4.36229151e-02 -2.77261862e-01\n  -1.63079250e-01  7.29233983e-03 -1.72117105e-01  4.75655924e-03\n   1.23890132e-01 -1.18386760e-01  1.30634993e-01 -7.34071373e-02\n  -3.07760726e-02 -1.70124153e-01  2.80205183e-02 -5.45700854e-02\n  -4.23629361e-04  1.39369855e-01  1.57196209e-01  1.37727931e-01\n  -4.20913770e-02 -4.01736957e-02  5.44814516e-02  2.28608684e-01\n   1.05157895e-01 -3.03991192e-02 -8.25245362e-02  2.13963693e-01\n   1.96254973e-01  1.72603471e-01 -1.00575750e-01  1.37847218e-01\n   1.48183807e-01  8.76461955e-02 -7.10796462e-02  4.50261974e-03\n  -9.82421621e-02 -2.79811862e-02  1.21870686e-01  3.15138412e-01\n   2.61449750e-01 -5.25685929e-02  1.79101271e-01 -4.69085317e-02\n   5.71823006e-02 -4.70725109e-02  2.60511487e-01  6.87893769e-02\n  -8.45613515e-02  1.08899777e-01  2.33440751e-02  1.90349415e-01\n   1.74695214e-03 -1.96588995e-01  1.11743120e-01  1.37657440e-01\n  -1.52259376e-02  1.55166430e-01  1.15364830e-01  1.20291704e-01\n   1.48216218e-01  1.78719580e-01  3.81469158e-01 -9.43610434e-02\n  -2.56093732e-01  9.95264680e-02  1.28319842e-01  1.78891685e-01\n   1.73643735e-02 -5.34219344e-02  1.53774422e-01  7.49980140e-03]]\nLayer 3 weights:\n[[ 1.71851885e-01  3.66106091e-01  3.61630508e-01 ... -9.42103687e-01\n   3.36414551e-01  1.29386426e-03]\n [-1.10055147e-01 -2.67703651e-01 -8.42270718e-01 ...  5.08878073e-02\n   5.63291358e-02  8.23561008e-01]\n [-2.87572854e-01 -8.21721698e-02  7.18071228e-01 ... -2.70691168e-01\n  -3.63883688e-01 -3.11778084e-02]\n ...\n [-2.59075464e-02  3.85191316e-01 -4.20262648e-01 ...  2.66749766e-01\n   2.20113897e-01 -7.10345996e-01]\n [ 5.09054314e-01 -7.02305707e-02  1.61157394e-01 ... -2.58477048e-01\n   7.02735692e-01 -1.37641733e+00]\n [ 8.37570562e-02 -2.78738204e-01  8.92349381e-02 ... -9.28571538e-02\n   2.46769938e-01  4.14102764e-01]]\nLayer 3 biases:\n[[-0.37280662  0.12273027 -0.21396619 -0.35079316  0.35577517  0.30238663\n  -0.18402651 -0.19292535  0.45398395  0.07964181]]\nEpoch 25/25, Loss: 0.00016345877635909982\nLayer 1 weights:\n[[ 5.93538986e-03  2.60148577e-03 -2.35444458e-03 ...  3.78851229e-03\n   1.47141819e-03  4.29591303e-03]\n [-4.41319004e-03  9.29479062e-03  3.61001946e-03 ... -9.82393811e-03\n   8.56413506e-03 -1.82450003e-03]\n [ 2.01436544e-03  1.63776871e-03  5.47264251e-04 ...  1.14379761e-03\n  -7.87677296e-03 -2.92317146e-03]\n ...\n [-8.91762530e-05 -1.87301902e-03 -2.37005061e-03 ... -2.09615579e-03\n   6.75959957e-03  5.09208878e-03]\n [-3.74598464e-03  6.73663176e-03  2.72120217e-02 ...  2.70637627e-03\n   3.14580157e-03  7.72825713e-04]\n [-1.67471550e-02 -1.94004075e-03 -1.75420087e-03 ...  8.10525999e-03\n  -1.54631799e-02  1.05056601e-02]]\nLayer 1 biases:\n[[-1.33379433e-01  1.00061888e-01  8.96422015e-02  6.13759650e-03\n  -2.01492045e-02  1.81981302e-02 -7.04851763e-02 -2.23824258e-03\n  -5.28752021e-02 -1.49399605e-01 -2.02722635e-01  6.60181420e-02\n   2.85645139e-01 -2.16076271e-02  2.73163296e-01 -1.38715312e-01\n  -8.50745065e-02 -9.24385907e-02  6.18167770e-03 -9.67659604e-04\n  -1.33513118e-01  9.28523130e-02  8.02236298e-05 -2.00461961e-01\n  -3.57267613e-02 -2.43009291e-02 -9.29933425e-02 -2.68262628e-01\n  -4.11933264e-02  2.72831653e-01 -5.84051591e-03  7.21829823e-02\n   1.42336852e-01  7.02956646e-02  1.30443238e-01  2.19153554e-02\n   2.43205871e-01  8.49732765e-02 -3.35949411e-02 -7.03415516e-02\n   2.15180046e-01  9.57747432e-02  7.29016285e-02 -5.25108450e-03\n  -1.33771772e-01 -4.74324937e-02  1.01602118e-01  1.78378948e-02\n  -2.06264760e-01 -5.03993679e-02 -1.05952767e-02 -1.15980540e-01\n  -1.81364329e-03  3.85834157e-02 -8.06724795e-02  1.53426359e-01\n  -9.42840534e-02  2.62185556e-01  4.55216870e-02 -2.78254084e-01\n  -1.65368305e-01  7.39184550e-03 -1.71751237e-01  4.76363937e-03\n   1.24318783e-01 -1.16855437e-01  1.30647278e-01 -7.30962041e-02\n  -3.09269500e-02 -1.70373991e-01  2.92935034e-02 -5.32637602e-02\n  -4.22746393e-04  1.41173098e-01  1.59259743e-01  1.37240898e-01\n  -4.31863542e-02 -3.99621166e-02  5.38011838e-02  2.29636547e-01\n   1.05510373e-01 -2.92679384e-02 -8.20756034e-02  2.14412032e-01\n   1.95975971e-01  1.73978828e-01 -1.02172787e-01  1.37911632e-01\n   1.49102965e-01  8.81586496e-02 -7.08317794e-02  5.16068012e-03\n  -9.86791883e-02 -2.74607930e-02  1.21220616e-01  3.13704662e-01\n   2.64273156e-01 -5.27961747e-02  1.81980014e-01 -4.73677289e-02\n   5.52085095e-02 -4.75060572e-02  2.64557512e-01  7.00796617e-02\n  -8.65280250e-02  1.08887210e-01  2.49883125e-02  1.94229160e-01\n   1.20721736e-03 -1.98014255e-01  1.13515847e-01  1.39195441e-01\n  -1.70731235e-02  1.53648748e-01  1.16826672e-01  1.21055006e-01\n   1.50162256e-01  1.81161129e-01  3.82268119e-01 -9.67165758e-02\n  -2.59056620e-01  1.00883679e-01  1.32168047e-01  1.79085959e-01\n   1.76758524e-02 -5.37882116e-02  1.53096126e-01  6.52031933e-03]]\nLayer 3 weights:\n[[ 1.72056576e-01  3.72043763e-01  3.65554896e-01 ... -9.50559516e-01\n   3.36176551e-01  3.63213729e-04]\n [-1.11386693e-01 -2.68245588e-01 -8.48003340e-01 ...  5.24855593e-02\n   5.42083671e-02  8.32195226e-01]\n [-2.89401229e-01 -8.27600463e-02  7.21436738e-01 ... -2.72218515e-01\n  -3.65522319e-01 -3.09665807e-02]\n ...\n [-2.52828205e-02  3.89853906e-01 -4.23559912e-01 ...  2.69008618e-01\n   2.20708227e-01 -7.18162143e-01]\n [ 5.15344224e-01 -7.28217886e-02  1.61802163e-01 ... -2.57613901e-01\n   7.05235313e-01 -1.38401356e+00]\n [ 8.41521958e-02 -2.79897386e-01  8.98316279e-02 ... -9.45912155e-02\n   2.47219832e-01  4.15957642e-01]]\nLayer 3 biases:\n[[-0.37235714  0.12451065 -0.21408864 -0.35203535  0.35617993  0.30264593\n  -0.18483796 -0.1940739   0.45376988  0.0802866 ]]\nValidation Loss: 0.09787394535273512\nValidation Accuracy: 0.9789285714285715\n","output_type":"stream"}]}]}